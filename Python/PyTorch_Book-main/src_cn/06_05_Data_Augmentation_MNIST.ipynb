{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST with Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 载入套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchmetrics import Accuracy\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设定参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 设定参数\n",
    "PATH_DATASETS = \"\" # 预设路径\n",
    "BATCH_SIZE = 1000  # 批量\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义资料增补函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_width = 28\n",
    "train_transforms = transforms.Compose([\n",
    "    #transforms.ColorJitter(), # 亮度、饱和度、对比资料增补\n",
    "    # 裁切部分图像，再调整图像尺寸\n",
    "    transforms.RandomResizedCrop(image_width, scale=(0.8, 1.0)), \n",
    "    transforms.RandomRotation(degrees=(-10, 10)), # 旋转 10 度\n",
    "    #transforms.RandomHorizontalFlip(), # 水平翻转\n",
    "    #transforms.RandomAffine(10), # 仿射\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "    ])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((image_width, image_width)), # 调整图像尺寸\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤1：载入 MNIST 手写阿拉伯数字资料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28]) torch.Size([10000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# 下载 MNIST 手写阿拉伯数字 训练资料\n",
    "train_ds = MNIST(PATH_DATASETS, train=True, download=True, \n",
    "                 transform=train_transforms)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "# 下载测试资料\n",
    "test_ds = MNIST(PATH_DATASETS, train=False, download=True,  \n",
    "                 transform=test_transforms)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=BATCH_SIZE,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "# 训练/测试资料的维度\n",
    "print(train_ds.data.shape, test_ds.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤2：资料清理，此步骤无需进行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤3：特征工程，此步骤无需进行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤4：资料分割，此步骤无需进行，载入MNIST资料时，已经切割好了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤5：建立模型结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv2d 参数： in-channel, out-channel, kernel size, Stride, Padding\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤6：结合训练资料及模型，进行模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    loss_list = []    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        #loss = F.nll_loss(output, target)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (batch_idx+1) % 10 == 0:\n",
    "            loss_list.append(loss.item())\n",
    "            batch = (batch_idx+1) * len(data)\n",
    "            data_count = len(train_loader.dataset)\n",
    "            percentage = (100. * (batch_idx+1) / len(train_loader))\n",
    "            print(f'Epoch {epoch}: [{batch:5d} / {data_count}] ({percentage:.0f} %)' +\n",
    "                  f'  Loss: {loss.item():.6f}')\n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            # print(data.shape)\n",
    "            # print(target)\n",
    "            if type(data) == tuple:\n",
    "                data = torch.FloatTensor(data)\n",
    "            if type(target) == tuple:\n",
    "                target = torch.Tensor(target)\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            #test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            # print(predicted)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "    # 平均损失\n",
    "    test_loss /= len(test_loader.dataset) \n",
    "    # 显示测试结果\n",
    "    data_count = len(test_loader.dataset)\n",
    "    percentage = 100. * correct / data_count \n",
    "    print(f'准确率: {correct}/{data_count} ({percentage:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: [10000 / 60000] (17 %)  Loss: 1.905932\n",
      "Epoch 1: [20000 / 60000] (33 %)  Loss: 0.823097\n",
      "Epoch 1: [30000 / 60000] (50 %)  Loss: 0.602738\n",
      "Epoch 1: [40000 / 60000] (67 %)  Loss: 0.445632\n",
      "Epoch 1: [50000 / 60000] (83 %)  Loss: 0.422776\n",
      "Epoch 1: [60000 / 60000] (100 %)  Loss: 0.350808\n",
      "Epoch 2: [10000 / 60000] (17 %)  Loss: 0.291289\n",
      "Epoch 2: [20000 / 60000] (33 %)  Loss: 0.363400\n",
      "Epoch 2: [30000 / 60000] (50 %)  Loss: 0.264314\n",
      "Epoch 2: [40000 / 60000] (67 %)  Loss: 0.250480\n",
      "Epoch 2: [50000 / 60000] (83 %)  Loss: 0.253205\n",
      "Epoch 2: [60000 / 60000] (100 %)  Loss: 0.235315\n",
      "Epoch 3: [10000 / 60000] (17 %)  Loss: 0.223402\n",
      "Epoch 3: [20000 / 60000] (33 %)  Loss: 0.223477\n",
      "Epoch 3: [30000 / 60000] (50 %)  Loss: 0.259600\n",
      "Epoch 3: [40000 / 60000] (67 %)  Loss: 0.182856\n",
      "Epoch 3: [50000 / 60000] (83 %)  Loss: 0.270474\n",
      "Epoch 3: [60000 / 60000] (100 %)  Loss: 0.182969\n",
      "Epoch 4: [10000 / 60000] (17 %)  Loss: 0.179848\n",
      "Epoch 4: [20000 / 60000] (33 %)  Loss: 0.159400\n",
      "Epoch 4: [30000 / 60000] (50 %)  Loss: 0.181598\n",
      "Epoch 4: [40000 / 60000] (67 %)  Loss: 0.177829\n",
      "Epoch 4: [50000 / 60000] (83 %)  Loss: 0.148128\n",
      "Epoch 4: [60000 / 60000] (100 %)  Loss: 0.147459\n",
      "Epoch 5: [10000 / 60000] (17 %)  Loss: 0.129847\n",
      "Epoch 5: [20000 / 60000] (33 %)  Loss: 0.142698\n",
      "Epoch 5: [30000 / 60000] (50 %)  Loss: 0.180243\n",
      "Epoch 5: [40000 / 60000] (67 %)  Loss: 0.129879\n",
      "Epoch 5: [50000 / 60000] (83 %)  Loss: 0.130637\n",
      "Epoch 5: [60000 / 60000] (100 %)  Loss: 0.104270\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "lr=1\n",
    "\n",
    "# 建立模型\n",
    "model = Net().to(device)\n",
    "\n",
    "# 损失\n",
    "criterion = F.nll_loss # nn.CrossEntropyLoss()\n",
    "\n",
    "# 设定优化器(optimizer)\n",
    "optimizer = torch.optim.Adadelta(model.parameters(), lr=lr)\n",
    "\n",
    "loss_list = []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    loss_list += train(model, device, train_loader, criterion, optimizer, epoch)\n",
    "    #test(model, device, test_loader)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1d52bb7bd90>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 对训练过程的损失绘图\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_list, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤7：评分(Score Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率: 9839/10000 (98.39%)\n"
     ]
    }
   ],
   "source": [
    "test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual    : [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]\n",
      "prediction:  7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4\n"
     ]
    }
   ],
   "source": [
    "# 实际预测 20 笔资料\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for i in range(20):\n",
    "        data, target = test_ds[i][0], test_ds[i][1]\n",
    "        data = data.reshape(1, *data.shape).to(device)\n",
    "        output = torch.argmax(model(data), axis=-1)\n",
    "        predictions.append(str(output.item()))\n",
    "\n",
    "# 比对\n",
    "print('actual    :', test_ds.targets[0:20].numpy())\n",
    "print('prediction: ', ' '.join(predictions[0:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤8：评估，暂不进行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤9：模型布署"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤10：新资料预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方法1：使用 Pillow 套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 显示图像\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def imshow(X):\n",
    "    # 绘制点阵图，cmap='gray':灰阶\n",
    "    plt.imshow(X.reshape(28,28), cmap='gray')\n",
    "\n",
    "    # 隐藏刻度\n",
    "    plt.axis('off') \n",
    "\n",
    "    # 显示图形\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual/prediction: 0 0\n",
      "actual/prediction: 1 1\n",
      "actual/prediction: 2 2\n",
      "actual/prediction: 3 3\n",
      "actual/prediction: 4 4\n",
      "actual/prediction: 5 5\n",
      "actual/prediction: 6 6\n",
      "actual/prediction: 7 3\n",
      "actual/prediction: 8 8\n",
      "actual/prediction: 9 8\n"
     ]
    }
   ],
   "source": [
    "# 使用PIL读取档案，像素介于[0, 255]\n",
    "import PIL.Image as Image\n",
    "\n",
    "data_shape = data.shape\n",
    "\n",
    "for i in range(10):\n",
    "    uploaded_file = f'./myDigits/{i}.png'\n",
    "    image1 = Image.open(uploaded_file).convert('L')\n",
    "\n",
    "    # 缩为 (28, 28) 大小的影像\n",
    "    image_resized = image1.resize(tuple(data_shape)[2:])\n",
    "    X1 = np.array(image_resized).reshape([1]+list(data_shape)[1:])\n",
    "    # 反转颜色，颜色0为白色，与 RGB 色码不同，它的 0 为黑色\n",
    "    X1 = 1.0-(X1/255)\n",
    "\n",
    "    # 图像转换\n",
    "    X1 = (X1 - 0.1307) / 0.3081  \n",
    "    \n",
    "    # 显示转换后的图像\n",
    "    # imshow(X1)\n",
    "    \n",
    "    X1 = torch.FloatTensor(X1).to(device)\n",
    "    \n",
    "    # 预测\n",
    "    output = model(X1)\n",
    "    # print(output, '\\n')\n",
    "    _, predicted = torch.max(output.data, 1)\n",
    "    print(f'actual/prediction: {i} {predicted.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方法2：使用 skimage 套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual/prediction: 0 0\n",
      "actual/prediction: 1 1\n",
      "actual/prediction: 2 2\n",
      "actual/prediction: 3 3\n",
      "actual/prediction: 4 4\n",
      "actual/prediction: 5 5\n",
      "actual/prediction: 6 6\n",
      "actual/prediction: 7 3\n",
      "actual/prediction: 8 8\n",
      "actual/prediction: 9 9\n"
     ]
    }
   ],
   "source": [
    "# 使用 skimage 读取档案，像素介于[0, 1]\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "\n",
    "# 读取影像并转为单色\n",
    "for i in range(10):\n",
    "    uploaded_file = f'./myDigits/{i}.png'\n",
    "    image1 = io.imread(uploaded_file, as_gray=True)\n",
    "\n",
    "    # 缩为 (28, 28) 大小的影像\n",
    "    image_resized = resize(image1, tuple(data_shape)[2:], anti_aliasing=True)    \n",
    "    X1 = image_resized.reshape([1]+list(data_shape)[1:]) \n",
    "    # 反转颜色，颜色0为白色，与 RGB 色码不同，它的 0 为黑色\n",
    "    X1 = 1.0-X1\n",
    "    \n",
    "    # 图像转换\n",
    "    X1 = (X1 - 0.1307) / 0.3081  \n",
    "\n",
    "    # 显示转换后的图像\n",
    "    # imshow(X1)\n",
    "    \n",
    "    X1 = torch.FloatTensor(X1).to(device)\n",
    "    \n",
    "    # 预测\n",
    "    output = model(X1)\n",
    "    _, predicted = torch.max(output.data, 1)\n",
    "    print(f'actual/prediction: {i} {predicted.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方法3：使用自订资料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_dir, transform=None, target_transform=None\n",
    "                 , to_gray=False, size=28):\n",
    "        self.img_labels = [file_name for file_name in os.listdir(img_dir)]\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.to_gray = to_gray\n",
    "        self.size = size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 组合档案完整路径\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels[idx])\n",
    "        # 读取图档\n",
    "        mode = 'L' if self.to_gray else 'RGB'\n",
    "        image = Image.open(img_path, mode='r').convert(mode)\n",
    "        image = Image.fromarray(1.0-(np.array(image)/255))\n",
    "\n",
    "        # print(image.shape)\n",
    "        # 去除副档名\n",
    "        label = int(self.img_labels[idx].split('.')[0])\n",
    "        \n",
    "        # 转换\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率: 9/10 (90.00%)\n"
     ]
    }
   ],
   "source": [
    "ds = CustomImageDataset('./myDigits', to_gray=True, transform=test_transforms)\n",
    "data_loader = torch.utils.data.DataLoader(ds, batch_size=10,shuffle=False)\n",
    "\n",
    "test(model, device, data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 3, 8, 9], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in data_loader:\n",
    "        print(target)\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # 预测\n",
    "        output = model(data)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        correct += (predicted == target).sum().item()\n",
    "        print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型存档\n",
    "torch.save(model, 'cnn_augmentation_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
