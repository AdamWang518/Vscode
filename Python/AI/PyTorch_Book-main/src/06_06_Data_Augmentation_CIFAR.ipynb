{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR with Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 載入套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchmetrics import Accuracy\n",
    "import torchvision \n",
    "from torchvision import transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 設定參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 設定參數\n",
    "BATCH_SIZE = 1000  # 批量\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定義資料增補函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_width = 32\n",
    "train_transforms = transforms.Compose([\n",
    "    # 裁切部分圖像，再調整圖像尺寸\n",
    "    #transforms.RandomResizedCrop(image_width, scale=(0.8, 1.0)), \n",
    "    #transforms.RandomRotation(degrees=(-10, 10)), # 旋轉 10 度\n",
    "    #transforms.RandomHorizontalFlip(), # 水平翻轉\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomCrop(image_width, padding=4),\n",
    "    #transforms.ColorJitter(), # 亮度、飽和度、對比資料增補\n",
    "    #transforms.RandomAffine(10), # 仿射\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "    ])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((image_width, image_width)), # 調整圖像尺寸\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步驟1：載入資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "(50000, 32, 32, 3) (10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# 載入資料集，如果出現 BrokenPipeError 錯誤，將 num_workers 改為 0\n",
    "train_ds = torchvision.datasets.CIFAR10(root='./CIFAR10', train=True,\n",
    "                                download=True, transform=train_transforms)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE,\n",
    "                                shuffle=True, num_workers=2)\n",
    "\n",
    "test_ds = torchvision.datasets.CIFAR10(root='./CIFAR10', train=False,\n",
    "                                download=True, transform=test_transforms)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=BATCH_SIZE,\n",
    "                                shuffle=False , num_workers=2)\n",
    "\n",
    "# 訓練/測試資料的維度\n",
    "print(train_ds.data.shape, test_ds.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步驟2：資料清理，此步驟無需進行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步驟3：特徵工程，此步驟無需進行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步驟4：資料分割，此步驟無需進行，載入MNIST資料時，已經切割好了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步驟5：建立模型結構"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 顏色要放在第1維，3:RGB三顏色\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5) \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "#         output = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步驟6：結合訓練資料及模型，進行模型訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    loss_list = []    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        #loss = F.nll_loss(output, target)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (batch_idx+1) % 10 == 0:\n",
    "            loss_list.append(loss.item())\n",
    "            batch = (batch_idx+1) * len(data)\n",
    "            data_count = len(train_loader.dataset)\n",
    "            percentage = (100. * (batch_idx+1) / len(train_loader))\n",
    "            print(f'Epoch {epoch}: [{batch:5d} / {data_count}] ({percentage:.0f} %)' +\n",
    "                  f'  Loss: {loss.item():.6f}')\n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            # print(data.shape)\n",
    "            # print(target)\n",
    "            if type(data) == tuple:\n",
    "                data = torch.FloatTensor(data)\n",
    "            if type(target) == tuple:\n",
    "                target = torch.Tensor(target)\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            #test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            # print(predicted)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "    # 平均損失\n",
    "    test_loss /= len(test_loader.dataset) \n",
    "    # 顯示測試結果\n",
    "    data_count = len(test_loader.dataset)\n",
    "    percentage = 100. * correct / data_count \n",
    "    print(f'準確率: {correct}/{data_count} ({percentage:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: [10000 / 50000] (20 %)  Loss: 2.300905\n",
      "Epoch 1: [20000 / 50000] (40 %)  Loss: 2.302145\n",
      "Epoch 1: [30000 / 50000] (60 %)  Loss: 2.295762\n",
      "Epoch 1: [40000 / 50000] (80 %)  Loss: 2.296967\n",
      "Epoch 1: [50000 / 50000] (100 %)  Loss: 2.280695\n",
      "Epoch 2: [10000 / 50000] (20 %)  Loss: 2.279377\n",
      "Epoch 2: [20000 / 50000] (40 %)  Loss: 2.274392\n",
      "Epoch 2: [30000 / 50000] (60 %)  Loss: 2.248450\n",
      "Epoch 2: [40000 / 50000] (80 %)  Loss: 2.202183\n",
      "Epoch 2: [50000 / 50000] (100 %)  Loss: 2.185362\n",
      "Epoch 3: [10000 / 50000] (20 %)  Loss: 2.127601\n",
      "Epoch 3: [20000 / 50000] (40 %)  Loss: 2.066868\n",
      "Epoch 3: [30000 / 50000] (60 %)  Loss: 1.988520\n",
      "Epoch 3: [40000 / 50000] (80 %)  Loss: 2.027357\n",
      "Epoch 3: [50000 / 50000] (100 %)  Loss: 1.972401\n",
      "Epoch 4: [10000 / 50000] (20 %)  Loss: 1.930738\n",
      "Epoch 4: [20000 / 50000] (40 %)  Loss: 1.944878\n",
      "Epoch 4: [30000 / 50000] (60 %)  Loss: 1.929402\n",
      "Epoch 4: [40000 / 50000] (80 %)  Loss: 1.947743\n",
      "Epoch 4: [50000 / 50000] (100 %)  Loss: 1.911450\n",
      "Epoch 5: [10000 / 50000] (20 %)  Loss: 1.877957\n",
      "Epoch 5: [20000 / 50000] (40 %)  Loss: 1.871954\n",
      "Epoch 5: [30000 / 50000] (60 %)  Loss: 1.891188\n",
      "Epoch 5: [40000 / 50000] (80 %)  Loss: 1.850889\n",
      "Epoch 5: [50000 / 50000] (100 %)  Loss: 1.844108\n",
      "Epoch 6: [10000 / 50000] (20 %)  Loss: 1.783884\n",
      "Epoch 6: [20000 / 50000] (40 %)  Loss: 1.790888\n",
      "Epoch 6: [30000 / 50000] (60 %)  Loss: 1.788255\n",
      "Epoch 6: [40000 / 50000] (80 %)  Loss: 1.753442\n",
      "Epoch 6: [50000 / 50000] (100 %)  Loss: 1.761248\n",
      "Epoch 7: [10000 / 50000] (20 %)  Loss: 1.752362\n",
      "Epoch 7: [20000 / 50000] (40 %)  Loss: 1.735809\n",
      "Epoch 7: [30000 / 50000] (60 %)  Loss: 1.736241\n",
      "Epoch 7: [40000 / 50000] (80 %)  Loss: 1.712714\n",
      "Epoch 7: [50000 / 50000] (100 %)  Loss: 1.709826\n",
      "Epoch 8: [10000 / 50000] (20 %)  Loss: 1.742376\n",
      "Epoch 8: [20000 / 50000] (40 %)  Loss: 1.707907\n",
      "Epoch 8: [30000 / 50000] (60 %)  Loss: 1.615485\n",
      "Epoch 8: [40000 / 50000] (80 %)  Loss: 1.689909\n",
      "Epoch 8: [50000 / 50000] (100 %)  Loss: 1.634977\n",
      "Epoch 9: [10000 / 50000] (20 %)  Loss: 1.777575\n",
      "Epoch 9: [20000 / 50000] (40 %)  Loss: 1.640873\n",
      "Epoch 9: [30000 / 50000] (60 %)  Loss: 1.626180\n",
      "Epoch 9: [40000 / 50000] (80 %)  Loss: 1.601874\n",
      "Epoch 9: [50000 / 50000] (100 %)  Loss: 1.640714\n",
      "Epoch 10: [10000 / 50000] (20 %)  Loss: 1.705840\n",
      "Epoch 10: [20000 / 50000] (40 %)  Loss: 1.643587\n",
      "Epoch 10: [30000 / 50000] (60 %)  Loss: 1.612452\n",
      "Epoch 10: [40000 / 50000] (80 %)  Loss: 1.595202\n",
      "Epoch 10: [50000 / 50000] (100 %)  Loss: 1.577840\n",
      "Epoch 11: [10000 / 50000] (20 %)  Loss: 1.588147\n",
      "Epoch 11: [20000 / 50000] (40 %)  Loss: 1.546586\n",
      "Epoch 11: [30000 / 50000] (60 %)  Loss: 1.611756\n",
      "Epoch 11: [40000 / 50000] (80 %)  Loss: 1.569436\n",
      "Epoch 11: [50000 / 50000] (100 %)  Loss: 1.511717\n",
      "Epoch 12: [10000 / 50000] (20 %)  Loss: 1.641429\n",
      "Epoch 12: [20000 / 50000] (40 %)  Loss: 1.513709\n",
      "Epoch 12: [30000 / 50000] (60 %)  Loss: 1.557309\n",
      "Epoch 12: [40000 / 50000] (80 %)  Loss: 1.587118\n",
      "Epoch 12: [50000 / 50000] (100 %)  Loss: 1.513070\n",
      "Epoch 13: [10000 / 50000] (20 %)  Loss: 1.527346\n",
      "Epoch 13: [20000 / 50000] (40 %)  Loss: 1.541386\n",
      "Epoch 13: [30000 / 50000] (60 %)  Loss: 1.556367\n",
      "Epoch 13: [40000 / 50000] (80 %)  Loss: 1.512127\n",
      "Epoch 13: [50000 / 50000] (100 %)  Loss: 1.483718\n",
      "Epoch 14: [10000 / 50000] (20 %)  Loss: 1.477342\n",
      "Epoch 14: [20000 / 50000] (40 %)  Loss: 1.477307\n",
      "Epoch 14: [30000 / 50000] (60 %)  Loss: 1.433045\n",
      "Epoch 14: [40000 / 50000] (80 %)  Loss: 1.456010\n",
      "Epoch 14: [50000 / 50000] (100 %)  Loss: 1.483551\n",
      "Epoch 15: [10000 / 50000] (20 %)  Loss: 1.495221\n",
      "Epoch 15: [20000 / 50000] (40 %)  Loss: 1.501765\n",
      "Epoch 15: [30000 / 50000] (60 %)  Loss: 1.524795\n",
      "Epoch 15: [40000 / 50000] (80 %)  Loss: 1.468676\n",
      "Epoch 15: [50000 / 50000] (100 %)  Loss: 1.391355\n",
      "Epoch 16: [10000 / 50000] (20 %)  Loss: 1.425697\n",
      "Epoch 16: [20000 / 50000] (40 %)  Loss: 1.532963\n",
      "Epoch 16: [30000 / 50000] (60 %)  Loss: 1.484824\n",
      "Epoch 16: [40000 / 50000] (80 %)  Loss: 1.456359\n",
      "Epoch 16: [50000 / 50000] (100 %)  Loss: 1.412084\n",
      "Epoch 17: [10000 / 50000] (20 %)  Loss: 1.480505\n",
      "Epoch 17: [20000 / 50000] (40 %)  Loss: 1.461053\n",
      "Epoch 17: [30000 / 50000] (60 %)  Loss: 1.467425\n",
      "Epoch 17: [40000 / 50000] (80 %)  Loss: 1.462530\n",
      "Epoch 17: [50000 / 50000] (100 %)  Loss: 1.375865\n",
      "Epoch 18: [10000 / 50000] (20 %)  Loss: 1.410266\n",
      "Epoch 18: [20000 / 50000] (40 %)  Loss: 1.413423\n",
      "Epoch 18: [30000 / 50000] (60 %)  Loss: 1.436381\n",
      "Epoch 18: [40000 / 50000] (80 %)  Loss: 1.474250\n",
      "Epoch 18: [50000 / 50000] (100 %)  Loss: 1.431102\n",
      "Epoch 19: [10000 / 50000] (20 %)  Loss: 1.442188\n",
      "Epoch 19: [20000 / 50000] (40 %)  Loss: 1.443574\n",
      "Epoch 19: [30000 / 50000] (60 %)  Loss: 1.350798\n",
      "Epoch 19: [40000 / 50000] (80 %)  Loss: 1.350612\n",
      "Epoch 19: [50000 / 50000] (100 %)  Loss: 1.376872\n",
      "Epoch 20: [10000 / 50000] (20 %)  Loss: 1.346630\n",
      "Epoch 20: [20000 / 50000] (40 %)  Loss: 1.357508\n",
      "Epoch 20: [30000 / 50000] (60 %)  Loss: 1.378163\n",
      "Epoch 20: [40000 / 50000] (80 %)  Loss: 1.380169\n",
      "Epoch 20: [50000 / 50000] (100 %)  Loss: 1.377262\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "lr=0.01\n",
    "\n",
    "# 建立模型\n",
    "model = Net().to(device)\n",
    "\n",
    "# 定義損失函數\n",
    "# 注意，nn.CrossEntropyLoss是類別，要先建立物件，要加 ()，其他損失函數不需要\n",
    "criterion = nn.CrossEntropyLoss() # F.nll_loss \n",
    "\n",
    "# 設定優化器(optimizer)\n",
    "#optimizer = torch.optim.Adadelta(model.parameters(), lr=lr)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "loss_list = []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    loss_list += train(model, device, train_loader, criterion, optimizer, epoch)\n",
    "    #test(model, device, test_loader)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x160625b7970>]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAn9UlEQVR4nO3deXyU1dn/8c8FCIrIIwgCAgoqQhCFQNywLCKu1eKKVaxrXSgq1rU/bbVqF1ttrbYqBVyLgGutWldEQEHwAUEWUYGKrIUICrIoAuf3x5V5ZpJMkplkMncy+b5fL1535p57Zs507HfOXPe5z7EQAiIiUvvVi7oBIiKSGQp0EZEcoUAXEckRCnQRkRyhQBcRyRENonrhFi1ahA4dOkT18iIitdKsWbO+DCG0THZfZIHeoUMHZs6cGdXLi4jUSmb2RVn3qeQiIpIjFOgiIjlCgS4ikiMU6CIiOUKBLiKSIxToIiI5QoEuIpIjIhuHXmlffgkffQSFhbB2LWzdCqefDgcdFHXLREQiVfsCfeJEOOec4vt+8Qs46SS44gpo3NhDf8sWOPNM2HPPSJopIpJttS/Q+/eHSZNg772hZUvYtg1Gj4aHH4bTTit+7KhRMGECNGkSQUNFRLLLolqxqKCgIGT00v9t22DKFGjY0IN+7lw47zw47jh46SXfLyJSy5nZrBBCQbL7al8PvSwNG8LAgfHbeXmwaRP89Kdw0UXw5JPQIHferohISbk9yuXSS+H3v4dx4+I9927d4PLLvRSzfXvULRQRyZjc77LefDN06uQlmLVrYeVKD/hRo6BFC7j/fi/NiIjUcrlTQ0/H1q3w+uvw29/CokX+b++9o2mLiEgayquh53bJpSy77eZj18eM8eGNt90WdYtERKqsbgZ6TJcuMGyYl1/mzo26NSIiVVK3Ax3g9tuhWTO49lqIqPwkIpIJCvRmzeDOO+Gdd+DFF6NujYhIpSnQwYcxdu4M99wTdUtERCpNgQ5+wdFPfgLvvw+rVkXdGhGRSlGgx5xxhm9VdhGRWkqBHpOX56Nenn8+6paIiFSKAj3RmWfC5Mk+/a6ISC2jQE90xhmwYwe8/HLULRERSZsCPVF+Puy3H7zwQtQtERFJW4WBbmbtzewdM1toZgvMbHiSY4aY2dyif9PMrHv1NLeamXkv/c03YePGqFsjIpKWVHro24HrQwh5wJHAMDPrWuKYz4F+IYRDgbuAkZltZhadcYYvlvHqq1G3REQkLRUGeghhdQjhw6K/vwEWAm1LHDMthPBV0c3pQLtMNzRreveG1q3hscc0FYCI1Cpp1dDNrAOQD8wo57BLgdfKePzlZjbTzGYWFham89LZU68e3HCDl11uvTXq1oiIpCzlBS7MrAnwPHBtCCFpgdnMjsED/QfJ7g8hjKSoHFNQUFBzu7/XXQeffearHbVvD0OHRt0iEZEKpRToZrYLHuZPhRCSDgExs0OB0cBJIYR1mWtiBMzgwQd9daOrroK2beFHP4q6VSIi5UpllIsBjwALQwh/LuOYfYEXgJ+EED7LbBMj0qABPP00dO8OV1+terqI1Hip1NCPBn4CDDCzOUX/TjazK83syqJjbgP2Ah4quj+iteUybPfdfSbGZctgyZKoWyMiUq4KSy4hhPcAq+CYnwI/zVSjapQBA3w7cSIceGC0bRERKYeuFK1Ip06wzz4e6CIiNZgCvSJm3kufOFF1dBGp0RToqRgwAAoLYcGCqFsiIlImBXoqEuvoIiI1lAI9FfvtB/vvr0AXkRpNgZ6qAQNg0iSfL11EpAZSoKdqwADYsAHmzIm6JSIiSSnQU9W/v29VdhGRGkqBnqo2bXwhaQW6iNRQCvR0DBgA774L338fdUtEREpRoKejXz/YvBk+/DDqloiIlKJAT0efPr6dPDnadoiIJKFAT0fr1tC5M0yZEnVLRERKUaCnq18/r6NrPLqI1DAK9HT17QsbN8LcuVG3RESkGAV6uvr1863q6CJSwyjQ09Wunc/rokAXkRpGgV4Zfft6HX3nzqhbIiLyfxToldGvH6xbBx9/HHVLRET+jwK9MmJ1dA1fFJEaRIFeGR06eC1ddXQRqUEU6JVh5r30yZO1zqiI1BgK9Mo66ihYswZWroy6JSIigAK98nr08K0WvBCRGkKBXlndu3vpZfbsqFsiIgIo0CuvSRPo1EmBLiI1hgK9Knr0UMlFRGoMBXpV5OfD55/D119H3RIREQV6lcROjH70UaTNEBEBBXrV5Of7NrGOvnw53HQTbNsWTZtEpM5SoFdFq1a+ilFiHf0Pf4B77oFJk6JqlYjUUQr0qsrPj/fQv/0WnnrK/37rrejaJCJ1kgK9qnr08FkXv/sOXnzRT5A2b65AF5GsqzDQzay9mb1jZgvNbIGZDU9yjJnZA2a22MzmmlnP6mluDZSfD9u3w4IF8OijsO++cN11fqJ0zZqoWycidUgqPfTtwPUhhDzgSGCYmXUtccxJQKeif5cDD2e0lTVZbKTLv/4FEybAxRfDCSf4vgkTImuWiNQ9FQZ6CGF1COHDor+/ARYCbUscNgh4MrjpwJ5m1ibjra2JDjgA9tjDT4SGABdd5L12lV1EJMvSqqGbWQcgH5hR4q62wPKE2ysoHfqY2eVmNtPMZhYWFqbZ1BqqXj2f12XrVjj2WJ8rvX59//uttzS9rohkTcqBbmZNgOeBa0MIG0veneQhpZIshDAyhFAQQiho2bJlei2tyWJll0suie877jhYtQoWLoykSSJS96QU6Ga2Cx7mT4UQXkhyyAqgfcLtdsCqqjevljjrLA/w00+P7zvuON+++WY0bRKROieVUS4GPAIsDCH8uYzDXgIuKBrtciSwIYSwOoPtrNn69fPg3m23+L4OHXw2RtXRRSRLGqRwzNHAT4B5ZjanaN8twL4AIYQRwKvAycBiYAtwccZbWhsddxw88YRPA9CwYdStEZEcV2GghxDeI3mNPPGYAAzLVKNyxgknwEMP+TQAxx8fdWtEJMfpStHqdPzx0LQpjB0bdUtEpA5QoFenXXf1E6bPPw9btkTdGhHJcQr06jZkCGzaBC+/HHVLRCTHKdCrW79+0LZtfBbGimzYoIuRRKRSFOjVrX59OPdceO01+PJL3/fNN3DnnbB0afFjZ82CffaB22/PejNFpPZToGfDkCE+I+Ozz8L69TBwoIf2Mcf4CkfgMzOedprX2h96yOdWFxFJgwI9G7p3h4MPhlGjPMTnzIG77/ZwP/ZYWLbMT56uWwf33uvb556LutUiUsso0LPBzHvps2fDokXwyitw881ehlm1CvLy4L334LHHfC71gw6Ch+vODMQikhkK9Gy5+GI49VR44434PC+9e8NLL3ng33ornHOO/33FFTBtGsydG22bRaRWsRDRiIqCgoIwc+bMSF67xvnuO2jUKH57/XofGXPRReqpi0gxZjYrhFCQ7D710GuCxDAHXxzjnHNgzBgfESMikgIFek01dKhfkDRmTNQtEZFaQoFeUx1+OPTsCX/9K+zcGXVrRKQWUKDXVGbw85/7ikdvvBF1a0SkFlCg12SDB/uVo/fdF3VLRKQWUKDXZA0bwtVX+6pH8+YlP+ajj2D69Oy2S0RqJAV6TXf55dC4cfJe+qJF0LcvnHKKr4okInWaAr2ma97cx6M/9ZTP9xKzZQuceSZs3epTBbz2WmRNFJGaQYFeGwwf7j3wK6+E+fN9et2hQ/3vF16Avff2tUtFpE5ToNcGBx0Ev/wlvPoqHHIIHHooPPkk3Habl1uGDPH5Ydati7qlIhIhBXptcdddsHKlz8a4fbtPtfurX/l9F14I338PTz8daRNFJFqayyVXdO/ua5jOmBF1S0SkGmkul7rgwgvhgw/gk0+ibomIRESBnivOO8+Xu/vHP6JuiYhERIGeK1q3hhNP9Br7ZZeppy5SBynQc8moUb6QxpgxvgrSFVdE3SIRySIFei5p0wZGjIAvvvChjCNHwurVUbdKRLJEgZ6L9t4brr3W/548OdKmiEj2KNBzVY8esMceCnSROkSBnqsaNIA+fRToInWIAj2X9evnC2QkTuolIjlLgZ7L+vXz7ZQp0bZDRLKiwkA3s0fNbK2ZzS/j/v8xs5fN7CMzW2BmF2e+mVIpPXvC7rur7CJSR6TSQ38cOLGc+4cBH4cQugP9gT+ZWcOqN02qbJdd4OijFegidUSFgR5CmAKsL+8QYA8zM6BJ0bHbM9M8qbL+/X3e9C+/jLolIlLNMlFD/xuQB6wC5gHDQwg7kx1oZpeb2Uwzm1lYWJiBl5YKxero774bbTtEpNplItBPAOYA+wA9gL+ZWdNkB4YQRoYQCkIIBS1btszAS0uFCgpgt91g0iRftu5vf4NBg+D66+Hxx+HTT6NuoYhkSIMMPMfFwN3BJ1ZfbGafA12ADzLw3FJVDRtC794wfjyMGweFhdCxI7z5Jnz7rc/QOG0aHH541C0VkSrKRA99GXAsgJm1AjoD/8nA80qmnHQSrF3rvfUpU+A//4FNm2DBAh8Fc//9UbdQRDIglWGL44D3gc5mtsLMLjWzK83syqJD7gJ6m9k84G3g5hCCzsDVJMOH+/J1r77qV4+C98y7dvXZGZ99tmZO4vXBB76e6ldfRd0SkVohlVEu54YQ2oQQdgkhtAshPBJCGBFCGFF0/6oQwvEhhENCCN1CCGOqv9mSlgYNYJ99kt83bJivR/r3v2e3TamYOhUWLfKrXUWkQrpStK7r1AlOPtkDfdu2qFtT3KpVvl2xItp2iNQSCnSBq6+G//4XnnvOby9cCLffDh9+GG27Vq70rQJdJCUKdIHjj/da9b33wiWXQLducOedcNhhXpKJqoatQBdJiwJdoF49uOoqmD0bxo71xTEWLfJ9I0Z42E+alP12qeQikpZMjEOXXHD55X7y9JRToH1733f//d5jP/dcvxhpyhTo3j077Qkh3kOPbUWkXOqhi2vUCIYOjYd5TPfu8MYb0LSpj2dfujS9533iCV+8Ol1ffw1bt/rf6qGLpEQ9dKlY+/bw+uvwgx/ACSfALbfAd9/B9u1w5pnQqlXyx331ldfgd+6EM86AvfZK/TVj5ZYDDvBFr3fs8LHzIlIm9dAlNQcfDC+9BMuXw0UXwRVXeFifcoqHbTJ//zts3uw97dGj03u9WJnl8MP9i2Pt2io1X6QuUKBL6vr08fLHkiUeuI8/DjNn+oRfJX33ndfgjz8ejjkGHnzQgzlViYEOKruIpECBLulp3hz239+vPL3gAq+r33orLFtW/LinnvKx7Tfe6FMPLF8OL76Y+uvESi6xQNeJUZEKKdCl8szgoYd8RMpVV/kWvGZ+773Qowcce6yXZTp2hAceSP25V670L48DD/Tb6qGLVEiBLlXToYNfhPTyy3DzzfDWWz5N78KFcMMNHvr163vgv/uuj3VPxcqV0LYttGjhUwAr0EUqpECXqhs+3Ee/3HOP18zPP99HxgweHD/mkkt8qt5Ue+mrVnmg16vn5Z3KBPrmzWWfsBXJQQp0qboGDXxY47p1MGGCl1vGjfNFqmP23NOHLr72WmrPuXJlfIbIdu3SD/Tvv/da/5/+lN7jRGoxBbpkTvPmXjO//no4+ujS9x96KKxZU/HcMNu3+3Ft2/rtdu3SPyk6e7YPdXz11fQeJ1KLKdAle/LyfFvR/OZr1viJ1cRAX7EiftI1FdOm+XbGDB9CKVIHKNAle2KB/vHHxfe/9prX23fu9Nux3nhiyeXbb2H9+tRfa+pU3377rY+VF6kDFOiSPfvtB7vuWrqHPmaML4M3b57fjgV6Yg8dUq+jh+A99OOO89vvvlu1dovUEgp0yZ769aFz59KBPmuWb996y7exi4pigR7bphroy5b5c5x2GnTpokCXOkOBLtnVtWvxQP/mG/jsM//7zTd9u3Klj5xp2dJvx3roqZ4YjZVbevf26QqmTtXwRakTFOiSXXl5Pnvi5s1+e/ZsL5F07uw96W+/9eBu08bHoAO0bu1/p9pDnzYNmjTxlZf69IENG2D+/Op5PyI1iAJdsisvzwP800/9dqzcctNNHubvvRe/qCimQQMP+FQDfepUOPJIf1yfPr5PZRepAxTokl0lhy7OnOnhPXiwX4j01lvFLyqKSfXiom++gblzvdwCfiK2XTsFutQJCnTJrk6d/ORoLNBnzYJevbxE0ru319Fj87gkSjXQP/jAhz/GLmwy8176u++mN45dpBZSoEt2NWzoqxAtXBg/Idqrl9933HEwZw5s3Fg60Nu2Te2k6NSpHuJHHBHf16cPrF4N//lPxt6GSE2kQJfsi410iZ0QTQz0mGQll40b/V95pk3zk6H/8z/xfaqjSx2hQJfsy8uDRYtg+nS/HQv0Xr2gWTP/O1nJBeJDHJPZsQPef7/0PDJdu/o0vOkssFEVhYU+B/yaNdl5PZEiCnTJvrw8n4DrmWe8J966te+vX98n94LSgX700T5j49lnw9KlyZ/37be9Bz9wYPH99erB0KHwr3+VnnagOkydCv/+t34RSNYp0CX7YiNdYidEE51/vtfY9923+P599/WpeTdsgH79ktfDH30U9trLe8clDR/u87HffXdm3kN5li/3bcll+USqmQJdsq9Ll/jfJQN90CBYvBh2263043r18lD/5hvo37/4qJf16+Gf//QvhEaNSj92r73giitg7Fj4/POMvI0yxYJcgS5ZpkCX7GvSxFc0AigoSO+xPXvCxInw5Zfe644ZOxa2bYOLLy77sddd52Wde+5Jv83pUA9dIqJAl2jEyi4le+ip6NEDfvlLeOGF+AIWjz7qz9W9e9mPa9sWLrrIj129Ov3XTVUs0GNbkSypMNDN7FEzW2tmZU6GYWb9zWyOmS0ws8mZbaLkpIED4bDD4idE03XDDV66ufpqH9kye3b5vfOYm27y5enuvLNyrzt/Ptx6a3zu9mRUcpGIpNJDfxw4saw7zWxP4CHgRyGEg4GzM9IyyW033uhXdVZWw4bw4IN+cnTQIK+bn3tuxY874AC45hoYMQJGjkz/dR96CH73O/8SSWb7dp+LplEjXwJv69b0X0OkkioM9BDCFKC8pWLOA14IISwrOn5thtomUr4BA2DIEB/3ffrpvqZpKu65B04+GX72M1/cOh2xpe3Gjk1+/+rV3nuPnRtId3FrkSrIRA39IKCZmU0ys1lmdkEGnlMkNX/6E/Tt6wtTp6pBAxg/3q8oHTzYJ/NKxcaNvqpSvXo+hv7770sfEyuz/OAHxW+LZEEmAr0B0Av4IXAC8CszOyjZgWZ2uZnNNLOZhYWFGXhpqfNatYLJk9MfLbPHHvDKK9C0Kfzwh/FVksozY4b3vocO9VE2b79d+pjYidDY1ao6MSpZlIlAXwG8HkLYHEL4EpgCJB1qEEIYGUIoCCEUtIytRiMSlXbtPNS/+gpOPTW+6Ma2bfDrX3udP9G0ad47//Wv/arVZGWXWIAfdZRPElZWDz0E+Otffcy9SIZkItD/BfQxswZm1hg4AlhYwWNEaoYePbz8MmeO1+PnzfOZGu+4A+69t/gVqVOnwiGH+LwwZ53lFzJt2VL8+ZYt84nBWrTwXw9lBfpbb/nJ2b/+tbremdRBqQxbHAe8D3Q2sxVmdqmZXWlmVwKEEBYCrwNzgQ+A0SEErfcltccpp8B99/lcL4ce6tP0Pvyw3/fUU77dscMnE4stnHHeebBpk/fwEy1fHr9oat99kwd6CN7Lh/iKTSIZ0KCiA0IIFY4FCyHcA1Tz5Xci1eiaa2DdOliyBP78Z9h7b++5P/WUX8Q0f75PORAL9L59fWKxsWP9xGpMyUBPtpbphAk+7LFNGx8/v2OHX8EqUkW6UlQk5o47YMwYD3PwEsynn8KHH8aHK8ZOdtavDz/+sV+p+vXX8edYtiw+sVish564UlKsd96+vV/ctGULfPJJdb8zqSMU6CJlOessv4BpzBgP9NatoUOH+P2DBvnQxclFF0dv3eqjX2I99PbtPbDXJ1zGMWGCP9ctt/iJU1DZRTJGgS5SlmbNfEjjuHE+t3nv3j5yJeaII3xWyIkT/XbsIqLEHjrE6+gh+K+Adu18moIuXaBx49QDfcsWnz5YpAwKdJHynH++rzz0xRelV0Jq1Mj3xQI9FtyJNfTE/bNn+0iZm27yx9avD/n5MHNmxe34/HOf0OyEE6r+niRnKdBFynPyyfH1SWMnRBMNGOAnPteujY9BLxnosf1PP+1XqQ4ZEn98r14+ZHLHjrLbsHQpHHOMfzHMmOF1fZEkFOgi5dl1Vz/5uccePhd7SQMG+HbSpHhwx9Y/bdnSe+KxE6PPPOMLYSfOOVNQUP6J0S++8DDfsMGHVQI8+2xG3prkHgW6SEXuvdfLIg0blr6vVy8P+4kTPbhbtYqvmGTmvfVly3xmyaVL4ZxzSj8ekpddlizxlZm++sovRPrRj3yOmGeeyeS7kxyiQBepSJMmcFDS6Ym8hNKvnwd64hj0mNjQxfHj/QvhtNOK39+5s691WvLE6IIF0KePj32fMCE+V83gwX4168IafjH2smU1v405SIEuUlUDBsCiRd7LTra49dKlXiY56aR4PT4m2YnRmTP9wiUoPfHYmWd6z7+ml12uuQaOP778hUAk4xToIlUVq6OvW5e8h756tU8nULLcEhM7Mbp9O7z3nj9f06Y+VPLgg4sfu88+3nPPZtll504fN1/Woh7JLFzowzg1xj6rFOgiVXXIIbDXXv53skAHH69+6qnJH19Q4BclPfSQD0ts08bD/IADkh8/eLCXZBYsyEz7KzJyJPz+9z7fTSp27PBhlhA/kStZoUAXqap69XwkCpQuucQC/pRTvBafTOzE6PDhsP/+MGVKfKRMMtksu6xY4ePmwed/T6WEsmJFfPEPBXpWKdBFMiFWdikZ6Acf7EMfL7207McedJBPK9Czpw9/bNWq/Ndq3dpPxI4Z42Wa6hKCL+axY4fPO7N+vZeGKhKb4/3UU32MfuIUxFKtFOgimXDhhV6aOOyw4vvbtvWl68q7wrN+fV8G7/3346Wbigwf7sMa//GPyre5Ik8/7dMD33UX/PSnvm/ChIoft2SJb6+7zrfqpWeNhcSZ4LKooKAgzEzlkmcRKS0En0tmzRr47LP42PdMWLfOv5z++Efo1Mm/aOrX9zVY27aFN94o//E33QQPPOAXTHXv7l9SkyZlrn11nJnNCiEkXXNRPXSR2sgMfvc7H+89cmRmnnPjRhg2zOv+t9zivzbGjo3P1T5woJ+s/fbb8p9nyRLo2NHPLQwa5I9Zty4zbZRyKdBFaqtjj/WTsb/5TXw91Mr6+GM4/HD4+999NaZ58+DNN+HAA+PHDBzoo3EqGr64eHF8hM6gQX4i9d//rlr7JCUKdJHaygx++1ufGOyBByr/PM8+62H+1Vc+kmX0aC+vlNSvn/fWy6ujh+A99NgXQa9ePnZedfSsUKCL1GZHHeWjSW691ed6mTCh+ApJFZk0yce1H3KIr8zUr1/Zx+6xh9ft33677GPWrPFfC7Eeer16cMYZfnK1rAWzwRcGefDB6h21Uwco0EVquyee8HVPp0/32RxjKyt17OjztW/ZkvxxIcCNN3rNfOJEP+FZkYED4X//t/iye4liI1wSSzU33OCvddddZT/vo4/CVVfBbbdV3IbKWreu7HbnCAW6SG3XrJmPE1+2DB5/3Odw79fPR5hMmwavvZb8cc895/PG3HWXX8maioEDvSZe1qiVWKAnXuW6335w5ZXw2GM+500y06f79ve/9958poXg5xvOPjvzz12DaNiiSK7avt173f37+5jyRN9/D127+kVPc+bER7JUZNs2n8/9kkuS1+1vu83r+lu3Fp9u+L//9ZAfNMhHziQKwdvZu7dfhLR0qZd/mjaFRx7xgD/8cA/jww4rvgxgqqZO9amHzXxWzFR+jdRQGrYoUhc1aODTBLzySulRMKNH+2iUu+9OPczBQ7pnz7In3VqyxK+WLTl3fOvWPgPj+PF+EVWiFSt8ArP+/f0E7c6d/gujXTsf075+Pdx/v9fvO3b0ETnpGjXKv7xCqPkzVVaBAl0klw0e7DX0V1+N79u0yRer7tvXyzPpys+Hjz5Kvmze4sXF6+eJbrzRe92/+lXx/TNm+PaII7wX/+ST3uYhQ/x15s3zk62jR/sKTmWVkMqyYYPPTnnBBd72cePSe3wtokAXyWV9+vjcMInT7d52mwfkH/9YufJFfr73+JPVw5csKXuWyObNfcqCl16KL9cHXj9v1Mhr/uCjdQoLvVd96KG+r1kznw+ndWufHyYdY8d6Ceiyy+Dcc331qBydX0aBLpLL6teHs87yC3s2bfLw/Mtf4Gc/8x5xZeTn+3b27OL7v/7aR5KU1UOH+ALZzz0X3zd9updxki3xV1K3bukH+ujR/mXRq5f/YoHS5xRyhAJdJNcNHuw91Bde8F5u27Y+mqSyunb18C0Z6MlGuJR00EHQo0f8F8P333s9/sgjU3vtbt28hp7qSkgffuj/LrvMf43st5+ffB0/Pn7Miy/6F82xx/rsmGeemdpz10AKdJFcd/TRvmjGsGEehiNGeC27snbZxS9EKhnosWlzy+uhg3/BTJ/u9fC5c31umFQD/eCDvb6+dGlqx48e7SdDY78MAH78Y3/dDz/06YFPPx3eecfbUa+ef/F9801qz1/DKNBFcl2s7LJpkwfbD39Y9efMz/dATxz2HOuh779/+Y+NjQV/7rniJ0RTEZuSIJWyy+LFPvb9nHNgzz2Lv369ej6MccQIP1n7xRc+tDF28dMnn6TWnhpGgS5SF/zsZ3DaaV4/z4T8fK+XJ57cXLLEfwnsvnv5jz3wQK+ZP/OM99Rbty69MEhZunb1bUWBvnOnz+HeqJHPSpmodWtfsLtxYx/S+cc/+q8OgLw83y5cmFp7apgGUTdARLKgSxf45z8z93w9e/p29mwP4xD8qtNOnVJ7/ODB8Itf+JdAnz6pj7Zp2tTr4BUF+ujRMHmyj5TZZ5/S9z/zjLe55JfPAQd4uNfSQFcPXUTSd+ihXraI1dGnTfO69Hnnpfb4WNll3br0R9tUNNJl5UovoxxzTNlL/zVunPyXRIMG/qWkQBeROqNxY+jcOR7o99/vderzz0/t8fvvDwVFV6+nekI0pls3r3HHFqIGv8hp7lzvmZ91lt83alTlxtnn5eVuoJvZo2a21szK/Y1jZoeZ2Q4zOytzzRORGis/30eKLF/uI0Muu6zi+nmiiy/2L4GCpNOSlK1bNw/s2IVNmzd7bb17d2/Dp5/6VLzlDZ8sT16el4K2bavc4yOUSg/9ceDE8g4ws/rAH4AKFhsUkZyRn+/zsNxxh9ejhw1L7/FDh3p5pEmT9B538MG+jZVdxozxdVXvu8+369b5l0Vl5eV5j7+smSFrsAoDPYQwBVhfwWFXA88DazPRKBGpBWInRh95xMdy77dfeo8389JNurp08fr9/Pn+RfLAA96W4cO9/l2ZMkuiWjzSpco1dDNrC5wOjEjh2MvNbKaZzSwsLKzqS4tIlHr0iP99zTXZe93ddvOhj/Pn++pJH3/sYV7VII/p3Nmfqy4GOvAX4OYQQpKp14oLIYwMIRSEEApatmyZgZcWkcg0b+4nN3v08KGH2dStGyxY4L3zvff2i4cypXFj/7VRCwM9E+PQC4Dx5t+OLYCTzWx7COHFDDy3iNRkzz/va41mqnecqm7dfFz9okU+HW+jRpl9/lo60qXKgR5C6Bj728weB15RmIvUEYlll2zq1s3r57vs4svbZVpens/vsmNH8gVAvvuu/C+REOD1133isSVLfHnAtm291t+zp//vVpX5dMqQyrDFccD7QGczW2Fml5rZlWZWDf8rioikIDany+DBPt1ApuXl+WRdX3xR+r7x4324ZXkXNz35pC8e8qtfwRtv+NDKCRPg5z/31Zhuvz3zbSaFHnoI4dxUnyyEcFGVWiMikorOneE3v0n9QqZ0JY50SZxsbPNmuOEGD/vf/Kb4NLwxixf7EM6+fX2lqMSx+atX+8VY7dpVS7N1paiI1D716sGtt6Y/VDJVZQ1dvPdeHzt/wgk+H0zJ+7dt81WRGjb08fElL7Rq08Z77rGVmDJMgS4iUlLz5j56JjGwV670mRnPPtvDunFj+O1viz/u9tt9krJRo6B9++y2GQW6iEhyJUe63HILbN8Of/gDtGjhUxKPG+cjbXbsgDvv9PsuuyyyVY80fa6ISDJdusATT/hVsGY+TPLmm6Fj0cC+66+Hv/3NZ3b8+mufrvf88zM353wlKNBFRJIZPNjLJ0uWeA/8pJO8lx7TqpUPmbzvPq+VP/EEXHBBdO1FgS4iktyAAR7o5bnlFh9zfuWVPvImYgp0EZHKatHCe+g1hE6KiojkCAW6iEiOUKCLiOQIBbqISI5QoIuI5AgFuohIjlCgi4jkCAW6iEiOsBBCNC9sVggkmT0+JS2ALzPYnNqiLr7vuvieoW6+77r4niH9971fCCHposyRBXpVmNnMEEJB1O3Itrr4vuvie4a6+b7r4nuGzL5vlVxERHKEAl1EJEfU1kAfGXUDIlIX33ddfM9QN993XXzPkMH3XStr6CIiUlpt7aGLiEgJCnQRkRxR6wLdzE40s0/NbLGZ/SLq9lQHM2tvZu+Y2UIzW2Bmw4v2Nzezt8xsUdG2WdRtrQ5mVt/MZpvZK0W3O5rZjKL3/bSZNYy6jZlkZnua2XNm9knRZ35UXfiszeznRf99zzezcWa2ay5+1mb2qJmtNbP5CfuSfr7mHijKt7lm1jOd16pVgW5m9YEHgZOArsC5ZtY12lZVi+3A9SGEPOBIYFjR+/wF8HYIoRPwdtHtXDQcSFhunT8A9xW976+ASyNpVfW5H3g9hNAF6I6/95z+rM2sLXANUBBC6AbUB35Mbn7WjwMnlthX1ud7EtCp6N/lwMPpvFCtCnTgcGBxCOE/IYRtwHhgUMRtyrgQwuoQwodFf3+D/x+8Lf5enyg67AngtEgaWI3MrB3wQ2B00W0DBgDPFR2SU+/bzJoCfYFHAEII20IIX1MHPmt8CczdzKwB0BhYTQ5+1iGEKcD6ErvL+nwHAU8GNx3Y08zapPpatS3Q2wLLE26vKNqXs8ysA5APzABahRBWg4c+sHeETasufwFuAnYW3d4L+DqEsL3odq595vsDhcBjRWWm0Wa2Ozn+WYcQVgL3AsvwIN8AzCK3P+tEZX2+Vcq42hbolmRfzo67NLMmwPPAtSGEjVG3p7qZ2SnA2hDCrMTdSQ7Npc+8AdATeDiEkA9sJsfKK8kU1YwHAR2BfYDd8XJDSbn0WaeiSv+917ZAXwG0T7jdDlgVUVuqlZntgof5UyGEF4p2r4n9/Craro2qfdXkaOBHZrYUL6cNwHvsexb9LIfc+8xXACtCCDOKbj+HB3yuf9YDgc9DCIUhhO+BF4De5PZnnaisz7dKGVfbAv1/gU5FZ8Ib4idRXoq4TRlXVDd+BFgYQvhzwl0vARcW/X0h8K9st606hRD+XwihXQihA/7ZTgwhDAHeAc4qOiyn3ncI4b/AcjPrXLTrWOBjcvyzxkstR5pZ46L/3mPvO2c/6xLK+nxfAi4oGu1yJLAhVppJSQihVv0DTgY+A5YAt0bdnmp6jz/Af2bNBeYU/TsZrye/DSwq2jaPuq3V+L9Bf+CVor/3Bz4AFgPPAo2ibl+G32sPYGbR5/0i0KwufNbAHcAnwHzgH0CjXPysgXH4eYLv8R74pWV9vnjJ5cGifJuHjwJK+bV06b+ISI6obSUXEREpgwJdRCRHKNBFRHKEAl1EJEco0EVEcoQCXUQkRyjQRURyxP8HI6YjrsUYyn4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 對訓練過程的損失繪圖\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_list, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步驟7：評分(Score Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "準確率: 4934/10000 (49.34%)\n"
     ]
    }
   ],
   "source": [
    "test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual    : [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]\n",
      "prediction:  7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4\n"
     ]
    }
   ],
   "source": [
    "# 實際預測 20 筆資料\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for i in range(20):\n",
    "        data, target = test_ds[i][0], test_ds[i][1]\n",
    "        data = data.reshape(1, *data.shape).to(device)\n",
    "        output = torch.argmax(model(data), axis=-1)\n",
    "        predictions.append(str(output.item()))\n",
    "\n",
    "# 比對\n",
    "print('actual    :', test_ds.targets[0:20].numpy())\n",
    "print('prediction: ', ' '.join(predictions[0:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步驟8：評估，暫不進行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步驟9：模型佈署"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步驟10：新資料預測"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方法1：使用 Pillow 套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 顯示圖像\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def imshow(X):\n",
    "    # 繪製點陣圖，cmap='gray':灰階\n",
    "    plt.imshow(X.reshape(28,28), cmap='gray')\n",
    "\n",
    "    # 隱藏刻度\n",
    "    plt.axis('off') \n",
    "\n",
    "    # 顯示圖形\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual/prediction: 0 0\n",
      "actual/prediction: 1 1\n",
      "actual/prediction: 2 2\n",
      "actual/prediction: 3 3\n",
      "actual/prediction: 4 4\n",
      "actual/prediction: 5 5\n",
      "actual/prediction: 6 6\n",
      "actual/prediction: 7 7\n",
      "actual/prediction: 8 8\n",
      "actual/prediction: 9 9\n"
     ]
    }
   ],
   "source": [
    "# 使用PIL讀取檔案，像素介於[0, 255]\n",
    "import PIL.Image as Image\n",
    "\n",
    "data_shape = data.shape\n",
    "\n",
    "for i in range(10):\n",
    "    uploaded_file = f'./myDigits/{i}.png'\n",
    "    image1 = Image.open(uploaded_file).convert('L')\n",
    "\n",
    "    # 縮為 (28, 28) 大小的影像\n",
    "    image_resized = image1.resize(tuple(data_shape)[2:])\n",
    "    X1 = np.array(image_resized).reshape([1]+list(data_shape)[1:])\n",
    "    # 反轉顏色，顏色0為白色，與 RGB 色碼不同，它的 0 為黑色\n",
    "    X1 = 1.0-(X1/255)\n",
    "\n",
    "    # 圖像轉換\n",
    "    X1 = (X1 - 0.1307) / 0.3081  \n",
    "    \n",
    "    # 顯示轉換後的圖像\n",
    "    # imshow(X1)\n",
    "    \n",
    "    X1 = torch.FloatTensor(X1).to(device)\n",
    "    \n",
    "    # 預測\n",
    "    output = model(X1)\n",
    "    # print(output, '\\n')\n",
    "    _, predicted = torch.max(output.data, 1)\n",
    "    print(f'actual/prediction: {i} {predicted.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方法2：使用 skimage 套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual/prediction: 0 0\n",
      "actual/prediction: 1 1\n",
      "actual/prediction: 2 2\n",
      "actual/prediction: 3 3\n",
      "actual/prediction: 4 4\n",
      "actual/prediction: 5 5\n",
      "actual/prediction: 6 6\n",
      "actual/prediction: 7 7\n",
      "actual/prediction: 8 8\n",
      "actual/prediction: 9 9\n"
     ]
    }
   ],
   "source": [
    "# 使用 skimage 讀取檔案，像素介於[0, 1]\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "\n",
    "# 讀取影像並轉為單色\n",
    "for i in range(10):\n",
    "    uploaded_file = f'./myDigits/{i}.png'\n",
    "    image1 = io.imread(uploaded_file, as_gray=True)\n",
    "\n",
    "    # 縮為 (28, 28) 大小的影像\n",
    "    image_resized = resize(image1, tuple(data_shape)[2:], anti_aliasing=True)    \n",
    "    X1 = image_resized.reshape([1]+list(data_shape)[1:]) \n",
    "    # 反轉顏色，顏色0為白色，與 RGB 色碼不同，它的 0 為黑色\n",
    "    X1 = 1.0-X1\n",
    "    \n",
    "    # 圖像轉換\n",
    "    X1 = (X1 - 0.1307) / 0.3081  \n",
    "\n",
    "    # 顯示轉換後的圖像\n",
    "    # imshow(X1)\n",
    "    \n",
    "    X1 = torch.FloatTensor(X1).to(device)\n",
    "    \n",
    "    # 預測\n",
    "    output = model(X1)\n",
    "    _, predicted = torch.max(output.data, 1)\n",
    "    print(f'actual/prediction: {i} {predicted.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方法3：使用自訂資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_dir, transform=None, target_transform=None\n",
    "                 , to_gray=False, size=28):\n",
    "        self.img_labels = [file_name for file_name in os.listdir(img_dir)]\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.to_gray = to_gray\n",
    "        self.size = size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 組合檔案完整路徑\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels[idx])\n",
    "        # 讀取圖檔\n",
    "        mode = 'L' if self.to_gray else 'RGB'\n",
    "        image = Image.open(img_path, mode='r').convert(mode)\n",
    "        image = Image.fromarray(1.0-(np.array(image)/255))\n",
    "\n",
    "        # print(image.shape)\n",
    "        # 去除副檔名\n",
    "        label = int(self.img_labels[idx].split('.')[0])\n",
    "        \n",
    "        # 轉換\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "準確率: 10/10 (100.00%)\n"
     ]
    }
   ],
   "source": [
    "ds = CustomImageDataset('./myDigits', to_gray=True, transform=test_transforms)\n",
    "data_loader = torch.utils.data.DataLoader(ds, batch_size=10,shuffle=False)\n",
    "\n",
    "test(model, device, data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 驗證"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in data_loader:\n",
    "        print(target)\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # 預測\n",
    "        output = model(data)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        correct += (predicted == target).sum().item()\n",
    "        print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
