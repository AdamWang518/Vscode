{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 範例3. 使用CNN進行物件偵測\n",
    "### 修改自 [Turning any CNN image classifier into an object detector with Keras, TensorFlow, and OpenCV - PyImageSearch](https://www.pyimagesearch.com/2020/06/22/turning-any-cnn-image-classifier-into-an-object-detector-with-keras-tensorflow-and-opencv/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 載入套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import numpy as np\n",
    "import time\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 參數設定\n",
    "WIDTH = 600              # 圖像縮放為 (600, 600)\n",
    "PYR_SCALE = 1.5          # 影像金字塔縮放比例\n",
    "WIN_STEP = 16            # 視窗滑動步數\n",
    "ROI_SIZE = (250, 250)    # 視窗大小\n",
    "INPUT_SIZE = (224, 224)  # CNN的輸入尺寸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 載入 ResNet50 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet50(pretrained=True).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 讀取要辨識的圖片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 400)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "filename = './images_Object_Detection/bike.jpg'\n",
    "orig = Image.open(filename)\n",
    "# 等比例縮放圖片\n",
    "orig = orig.resize((WIDTH, int(orig.size[1] / orig.size[0] * WIDTH)))\n",
    "Width_Height_ratio = orig.size[1] / orig.size[0]\n",
    "orig.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定義滑動視窗與影像金字塔函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 滑動視窗函數        \n",
    "def sliding_window(image, step, ws):\n",
    "    for y in range(0, image.size[1] - ws[1], step):     # 向下滑動 stepSize 格\n",
    "        for x in range(0, image.size[0] - ws[0], step): # 向右滑動 stepSize 格\n",
    "            # 傳回裁剪後的視窗\n",
    "            yield (x, y, image.crop((x, y, x + ws[0], y + ws[1])))\n",
    "\n",
    "# 影像金字塔函數\n",
    "# image：原圖，scale：每次縮小倍數，minSize：最小尺寸\n",
    "def image_pyramid(image, scale=1.5, minSize=(224, 224)):\n",
    "    # 第一次傳回原圖\n",
    "    yield image\n",
    "\n",
    "    # keep looping over the image pyramid\n",
    "    while True:\n",
    "        # 計算縮小後的尺寸\n",
    "        w = int(image.size[0] / scale)\n",
    "        image = image.resize((w, int(Width_Height_ratio * w)))\n",
    "\n",
    "        # 直到最小尺寸為止\n",
    "        if image.size[0] < minSize[1] or image.size[1] < minSize[0]:\n",
    "            break\n",
    "\n",
    "        # 傳回縮小後的圖像\n",
    "        yield image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定義轉換函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 轉換函數\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(INPUT_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# PIL格式轉換為OpenCV格式\n",
    "def PIL2CV2(orig):\n",
    "    pil_image = orig.copy()\n",
    "    open_cv_image = np.array(pil_image) \n",
    "    return open_cv_image[:, :, ::-1].copy() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 經由影像金字塔與滑動視窗操作，取得每一個要偵測的視窗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 400) 1.0\n",
      "(400, 266) 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# 產生影像金字塔\n",
    "pyramid = image_pyramid(orig, scale=PYR_SCALE, minSize=ROI_SIZE)\n",
    "rois = torch.tensor([])    # 候選框\n",
    "locs = []    # 位置\n",
    "for image in pyramid:\n",
    "    # 框與原圖的比例\n",
    "    scale = WIDTH / float(image.size[0])\n",
    "    print(image.size, 1/scale)\n",
    "    \n",
    "    # 滑動視窗\n",
    "    for (x, y, roiOrig) in sliding_window(image, WIN_STEP, ROI_SIZE):\n",
    "        # 取得候選框\n",
    "        x = int(x * scale)\n",
    "        y = int(y * scale)\n",
    "        w = int(ROI_SIZE[0] * scale)\n",
    "        h = int(ROI_SIZE[1] * scale)\n",
    "\n",
    "        # 縮放圖形以符合模型輸入規格 \n",
    "        roi = transform(roiOrig)\n",
    "        roi = roi.unsqueeze(0) # 增加一維(筆數)\n",
    "\n",
    "        # 加入輸出變數中\n",
    "        if len(rois.shape) == 1:\n",
    "            rois = roi\n",
    "        else:\n",
    "            rois = torch.cat((rois, roi), dim=0)\n",
    "        locs.append((x, y, x + w, y + h))\n",
    "        \n",
    "rois = rois.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266.6666666666667"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "400 / 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0, 250, 250), (16, 0, 266, 250), (32, 0, 282, 250), (48, 0, 298, 250), (64, 0, 314, 250), (80, 0, 330, 250), (96, 0, 346, 250), (112, 0, 362, 250), (128, 0, 378, 250), (144, 0, 394, 250), (160, 0, 410, 250), (176, 0, 426, 250), (192, 0, 442, 250), (208, 0, 458, 250), (224, 0, 474, 250), (240, 0, 490, 250), (256, 0, 506, 250), (272, 0, 522, 250), (288, 0, 538, 250), (304, 0, 554, 250), (320, 0, 570, 250), (336, 0, 586, 250), (0, 16, 250, 266), (16, 16, 266, 266), (32, 16, 282, 266), (48, 16, 298, 266), (64, 16, 314, 266), (80, 16, 330, 266), (96, 16, 346, 266), (112, 16, 362, 266), (128, 16, 378, 266), (144, 16, 394, 266), (160, 16, 410, 266), (176, 16, 426, 266), (192, 16, 442, 266), (208, 16, 458, 266), (224, 16, 474, 266), (240, 16, 490, 266), (256, 16, 506, 266), (272, 16, 522, 266), (288, 16, 538, 266), (304, 16, 554, 266), (320, 16, 570, 266), (336, 16, 586, 266), (0, 32, 250, 282), (16, 32, 266, 282), (32, 32, 282, 282), (48, 32, 298, 282), (64, 32, 314, 282), (80, 32, 330, 282), (96, 32, 346, 282), (112, 32, 362, 282), (128, 32, 378, 282), (144, 32, 394, 282), (160, 32, 410, 282), (176, 32, 426, 282), (192, 32, 442, 282), (208, 32, 458, 282), (224, 32, 474, 282), (240, 32, 490, 282), (256, 32, 506, 282), (272, 32, 522, 282), (288, 32, 538, 282), (304, 32, 554, 282), (320, 32, 570, 282), (336, 32, 586, 282), (0, 48, 250, 298), (16, 48, 266, 298), (32, 48, 282, 298), (48, 48, 298, 298), (64, 48, 314, 298), (80, 48, 330, 298), (96, 48, 346, 298), (112, 48, 362, 298), (128, 48, 378, 298), (144, 48, 394, 298), (160, 48, 410, 298), (176, 48, 426, 298), (192, 48, 442, 298), (208, 48, 458, 298), (224, 48, 474, 298), (240, 48, 490, 298), (256, 48, 506, 298), (272, 48, 522, 298), (288, 48, 538, 298), (304, 48, 554, 298), (320, 48, 570, 298), (336, 48, 586, 298), (0, 64, 250, 314), (16, 64, 266, 314), (32, 64, 282, 314), (48, 64, 298, 314), (64, 64, 314, 314), (80, 64, 330, 314), (96, 64, 346, 314), (112, 64, 362, 314), (128, 64, 378, 314), (144, 64, 394, 314), (160, 64, 410, 314), (176, 64, 426, 314), (192, 64, 442, 314), (208, 64, 458, 314), (224, 64, 474, 314), (240, 64, 490, 314), (256, 64, 506, 314), (272, 64, 522, 314), (288, 64, 538, 314), (304, 64, 554, 314), (320, 64, 570, 314), (336, 64, 586, 314), (0, 80, 250, 330), (16, 80, 266, 330), (32, 80, 282, 330), (48, 80, 298, 330), (64, 80, 314, 330), (80, 80, 330, 330), (96, 80, 346, 330), (112, 80, 362, 330), (128, 80, 378, 330), (144, 80, 394, 330), (160, 80, 410, 330), (176, 80, 426, 330), (192, 80, 442, 330), (208, 80, 458, 330), (224, 80, 474, 330), (240, 80, 490, 330), (256, 80, 506, 330), (272, 80, 522, 330), (288, 80, 538, 330), (304, 80, 554, 330), (320, 80, 570, 330), (336, 80, 586, 330), (0, 96, 250, 346), (16, 96, 266, 346), (32, 96, 282, 346), (48, 96, 298, 346), (64, 96, 314, 346), (80, 96, 330, 346), (96, 96, 346, 346), (112, 96, 362, 346), (128, 96, 378, 346), (144, 96, 394, 346), (160, 96, 410, 346), (176, 96, 426, 346), (192, 96, 442, 346), (208, 96, 458, 346), (224, 96, 474, 346), (240, 96, 490, 346), (256, 96, 506, 346), (272, 96, 522, 346), (288, 96, 538, 346), (304, 96, 554, 346), (320, 96, 570, 346), (336, 96, 586, 346), (0, 112, 250, 362), (16, 112, 266, 362), (32, 112, 282, 362), (48, 112, 298, 362), (64, 112, 314, 362), (80, 112, 330, 362), (96, 112, 346, 362), (112, 112, 362, 362), (128, 112, 378, 362), (144, 112, 394, 362), (160, 112, 410, 362), (176, 112, 426, 362), (192, 112, 442, 362), (208, 112, 458, 362), (224, 112, 474, 362), (240, 112, 490, 362), (256, 112, 506, 362), (272, 112, 522, 362), (288, 112, 538, 362), (304, 112, 554, 362), (320, 112, 570, 362), (336, 112, 586, 362), (0, 128, 250, 378), (16, 128, 266, 378), (32, 128, 282, 378), (48, 128, 298, 378), (64, 128, 314, 378), (80, 128, 330, 378), (96, 128, 346, 378), (112, 128, 362, 378), (128, 128, 378, 378), (144, 128, 394, 378), (160, 128, 410, 378), (176, 128, 426, 378), (192, 128, 442, 378), (208, 128, 458, 378), (224, 128, 474, 378), (240, 128, 490, 378), (256, 128, 506, 378), (272, 128, 522, 378), (288, 128, 538, 378), (304, 128, 554, 378), (320, 128, 570, 378), (336, 128, 586, 378), (0, 144, 250, 394), (16, 144, 266, 394), (32, 144, 282, 394), (48, 144, 298, 394), (64, 144, 314, 394), (80, 144, 330, 394), (96, 144, 346, 394), (112, 144, 362, 394), (128, 144, 378, 394), (144, 144, 394, 394), (160, 144, 410, 394), (176, 144, 426, 394), (192, 144, 442, 394), (208, 144, 458, 394), (224, 144, 474, 394), (240, 144, 490, 394), (256, 144, 506, 394), (272, 144, 522, 394), (288, 144, 538, 394), (304, 144, 554, 394), (320, 144, 570, 394), (336, 144, 586, 394), (0, 0, 375, 375), (24, 0, 399, 375), (48, 0, 423, 375), (72, 0, 447, 375), (96, 0, 471, 375), (120, 0, 495, 375), (144, 0, 519, 375), (168, 0, 543, 375), (192, 0, 567, 375), (216, 0, 591, 375)]\n"
     ]
    }
   ],
   "source": [
    "print(locs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([230, 3, 224, 224])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rois.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取類別列表\n",
    "with open(\"imagenet_classes.txt\", \"r\") as f:\n",
    "    categories = [s.strip() for s in f.readlines()]\n",
    "\n",
    "# 預測\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(rois)\n",
    "    \n",
    "# 轉成機率\n",
    "probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "\n",
    "# 取得第一名\n",
    "top_prob, top_catid = torch.topk(probabilities, 1)\n",
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7631)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities[0, 518]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([0.6090]),\n",
       "indices=tensor([671]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(probabilities[202], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([518, 518, 518, 518, 518, 444, 671, 444, 444, 671, 444, 671, 671,\n",
       "       671, 671, 671, 671, 671, 671, 444, 671, 444, 444, 444, 444, 444,\n",
       "       444, 444, 444, 444, 444, 444, 444, 444, 444, 444, 444, 444, 444,\n",
       "       444, 444, 444, 444, 444, 444, 444, 444, 444, 444, 444, 444, 444,\n",
       "       444, 444, 444, 444, 444, 518, 444, 444, 671, 444, 444, 444, 444,\n",
       "       444, 518, 444, 444, 444, 518, 518, 671, 444, 444, 444, 444, 444,\n",
       "       518, 671, 671, 444, 671, 671, 444, 444, 444, 444, 518, 518, 518,\n",
       "       518, 671, 518, 518, 444, 444, 444, 444, 444, 671, 518, 518, 671,\n",
       "       671, 671, 444, 444, 444, 444, 518, 518, 671, 518, 671, 518, 518,\n",
       "       444, 444, 444, 444, 444, 671, 518, 518, 444, 671, 671, 444, 444,\n",
       "       444, 444, 444, 671, 671, 518, 671, 518, 671, 444, 671, 444, 444,\n",
       "       671, 671, 671, 444, 671, 671, 671, 444, 444, 444, 444, 518, 518,\n",
       "       671, 444, 671, 518, 518, 444, 444, 444, 444, 444, 671, 518, 671,\n",
       "       671, 671, 671, 444, 444, 444, 444, 671, 671, 671, 518, 518, 518,\n",
       "       518, 518, 518, 444, 444, 671, 518, 671, 671, 671, 671, 671, 444,\n",
       "       444, 444, 444, 518, 518, 671, 518, 671, 518, 518, 518, 444, 518,\n",
       "       671, 671, 671, 518, 671, 671, 671, 671, 671, 444, 671, 444, 671,\n",
       "       671, 671, 671, 671, 671, 671, 518, 444, 444], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_catid.numpy().reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.028649738058447838\n",
      "1 0.08130887150764465\n",
      "2 0.047475364059209824\n",
      "3 0.09942872822284698\n",
      "4 0.1982518881559372\n",
      "5 0.2799190878868103\n",
      "6 0.42887210845947266\n",
      "7 0.21747906506061554\n",
      "8 0.24788247048854828\n",
      "9 0.542870044708252\n",
      "10 0.44971707463264465\n",
      "11 0.5239542126655579\n",
      "12 0.6552249193191528\n",
      "13 0.5453253984451294\n",
      "14 0.7406800985336304\n",
      "15 0.5758123993873596\n",
      "16 0.7529944777488708\n",
      "17 0.7544339895248413\n",
      "18 0.4320701062679291\n",
      "19 0.3997156620025635\n",
      "20 0.3739883005619049\n",
      "21 0.35298678278923035\n",
      "22 0.0837104395031929\n",
      "23 0.10036090761423111\n",
      "24 0.0834302306175232\n",
      "25 0.04102570563554764\n",
      "26 0.047784529626369476\n",
      "27 0.14030373096466064\n",
      "28 0.1472254991531372\n",
      "29 0.033787939697504044\n",
      "30 0.19850336015224457\n",
      "31 0.05815694481134415\n",
      "32 0.14313583076000214\n",
      "33 0.17988049983978271\n",
      "34 0.29377245903015137\n",
      "35 0.38012439012527466\n",
      "36 0.13832098245620728\n",
      "37 0.18458841741085052\n",
      "38 0.21117062866687775\n",
      "39 0.3065633177757263\n",
      "40 0.20043738186359406\n",
      "41 0.053970929235219955\n",
      "42 0.11426839977502823\n",
      "43 0.15099339187145233\n",
      "44 0.29521122574806213\n",
      "45 0.25427111983299255\n",
      "46 0.13333922624588013\n",
      "47 0.082823246717453\n",
      "48 0.18034261465072632\n",
      "49 0.2352394014596939\n",
      "50 0.29831379652023315\n",
      "51 0.11044616252183914\n",
      "52 0.18329857289791107\n",
      "53 0.2020208239555359\n",
      "54 0.24493567645549774\n",
      "55 0.2766000032424927\n",
      "56 0.354826956987381\n",
      "57 0.23247802257537842\n",
      "58 0.3176829218864441\n",
      "59 0.3344135284423828\n",
      "60 0.5054400563240051\n",
      "61 0.4154697060585022\n",
      "62 0.17500953376293182\n",
      "63 0.05424285680055618\n",
      "64 0.03691355511546135\n",
      "65 0.09294506162405014\n",
      "66 0.2341824471950531\n",
      "67 0.2253466248512268\n",
      "68 0.20857638120651245\n",
      "69 0.13572558760643005\n",
      "70 0.3046497106552124\n",
      "71 0.2069803923368454\n",
      "72 0.4094606935977936\n",
      "73 0.22851406037807465\n",
      "74 0.10381914675235748\n",
      "75 0.15795142948627472\n",
      "76 0.16269716620445251\n",
      "77 0.3000005781650543\n",
      "78 0.3112509548664093\n",
      "79 0.34105154871940613\n",
      "80 0.48879820108413696\n",
      "81 0.42030906677246094\n",
      "82 0.663726806640625\n",
      "83 0.535589873790741\n",
      "84 0.22069862484931946\n",
      "85 0.08969591557979584\n",
      "86 0.09898049384355545\n",
      "87 0.04239309951663017\n",
      "88 0.12080475687980652\n",
      "89 0.3761718273162842\n",
      "90 0.3592129945755005\n",
      "91 0.1240251436829567\n",
      "92 0.4474141299724579\n",
      "93 0.24219372868537903\n",
      "94 0.29529792070388794\n",
      "95 0.10607200115919113\n",
      "96 0.1837727278470993\n",
      "97 0.13248516619205475\n",
      "98 0.28418031334877014\n",
      "99 0.28540802001953125\n",
      "100 0.47727757692337036\n",
      "101 0.1580878049135208\n",
      "102 0.2920060157775879\n",
      "103 0.42230895161628723\n",
      "104 0.47051966190338135\n",
      "105 0.6014961004257202\n",
      "106 0.34453901648521423\n",
      "107 0.17492935061454773\n",
      "108 0.08592263609170914\n",
      "109 0.15574762225151062\n",
      "110 0.251127690076828\n",
      "111 0.36083751916885376\n",
      "112 0.4938395917415619\n",
      "113 0.16554470360279083\n",
      "114 0.5363124012947083\n",
      "115 0.30547282099723816\n",
      "116 0.29592031240463257\n",
      "117 0.15512418746948242\n",
      "118 0.07852315157651901\n",
      "119 0.14547784626483917\n",
      "120 0.06536631286144257\n",
      "121 0.2659657597541809\n",
      "122 0.4197906255722046\n",
      "123 0.25075799226760864\n",
      "124 0.32029446959495544\n",
      "125 0.4110563397407532\n",
      "126 0.524316132068634\n",
      "127 0.6492218375205994\n",
      "128 0.18579557538032532\n",
      "129 0.14737387001514435\n",
      "130 0.13471898436546326\n",
      "131 0.07885225862264633\n",
      "132 0.35761702060699463\n",
      "133 0.5198532342910767\n",
      "134 0.4749496281147003\n",
      "135 0.14133387804031372\n",
      "136 0.5265132188796997\n",
      "137 0.26142504811286926\n",
      "138 0.4860256314277649\n",
      "139 0.190663680434227\n",
      "140 0.3412978947162628\n",
      "141 0.16874559223651886\n",
      "142 0.2027783840894699\n",
      "143 0.4088502824306488\n",
      "144 0.4132767617702484\n",
      "145 0.42442142963409424\n",
      "146 0.3677115738391876\n",
      "147 0.6908611059188843\n",
      "148 0.5623586177825928\n",
      "149 0.814322829246521\n",
      "150 0.3121257722377777\n",
      "151 0.17784206569194794\n",
      "152 0.0712960958480835\n",
      "153 0.139604851603508\n",
      "154 0.28260859847068787\n",
      "155 0.2730203866958618\n",
      "156 0.4887925088405609\n",
      "157 0.2118787169456482\n",
      "158 0.4913199543952942\n",
      "159 0.43166080117225647\n",
      "160 0.28558459877967834\n",
      "161 0.19129836559295654\n",
      "162 0.16857022047042847\n",
      "163 0.2931569516658783\n",
      "164 0.20273487269878387\n",
      "165 0.36546504497528076\n",
      "166 0.5474026799201965\n",
      "167 0.2992914915084839\n",
      "168 0.43801265954971313\n",
      "169 0.46452227234840393\n",
      "170 0.4597650468349457\n",
      "171 0.4145222008228302\n",
      "172 0.29957115650177\n",
      "173 0.2590150237083435\n",
      "174 0.2544363737106323\n",
      "175 0.19781173765659332\n",
      "176 0.476734459400177\n",
      "177 0.47613340616226196\n",
      "178 0.5771191716194153\n",
      "179 0.25939083099365234\n",
      "180 0.46244266629219055\n",
      "181 0.21359892189502716\n",
      "182 0.3821452558040619\n",
      "183 0.30286818742752075\n",
      "184 0.20972874760627747\n",
      "185 0.23280513286590576\n",
      "186 0.29928573966026306\n",
      "187 0.6788702011108398\n",
      "188 0.32284361124038696\n",
      "189 0.43918636441230774\n",
      "190 0.37321484088897705\n",
      "191 0.7288562059402466\n",
      "192 0.5593318343162537\n",
      "193 0.5628219842910767\n",
      "194 0.22479285299777985\n",
      "195 0.08962947875261307\n",
      "196 0.36174339056015015\n",
      "197 0.18854062259197235\n",
      "198 0.16250723600387573\n",
      "199 0.3196153938770294\n",
      "200 0.49827268719673157\n",
      "201 0.3268629312515259\n",
      "202 0.6089857220649719\n",
      "203 0.42064157128334045\n",
      "204 0.4241213798522949\n",
      "205 0.30227288603782654\n",
      "206 0.20149609446525574\n",
      "207 0.36131641268730164\n",
      "208 0.3595777750015259\n",
      "209 0.6038734912872314\n",
      "210 0.7197820544242859\n",
      "211 0.3746793270111084\n",
      "212 0.5886330008506775\n",
      "213 0.6049125790596008\n",
      "214 0.7958360910415649\n",
      "215 0.8280092477798462\n",
      "216 0.5865328311920166\n",
      "217 0.2116594910621643\n",
      "218 0.455513060092926\n",
      "219 0.34491854906082153\n",
      "220 0.4758867919445038\n",
      "221 0.6157715916633606\n",
      "222 0.4103216826915741\n",
      "223 0.5487216114997864\n",
      "224 0.5564377307891846\n",
      "225 0.5720820426940918\n",
      "226 0.3168434202671051\n",
      "227 0.1651475578546524\n",
      "228 0.17183753848075867\n",
      "229 0.2268025428056717\n"
     ]
    }
   ],
   "source": [
    "for i in range(probabilities.shape[0]):\n",
    "    print(i, probabilities[i, 671].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 檢查預測結果，辨識機率須大於設定值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['mountain bike'])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MIN_CONFIDENCE = 0.4  # 辨識機率門檻值\n",
    "\n",
    "labels = {}\n",
    "for (i, p) in enumerate(zip(top_prob.numpy().reshape(-1), \n",
    "                            top_catid.numpy().reshape(-1))):\n",
    "    (prob, imagenetID) = p\n",
    "    label = categories[imagenetID]\n",
    "\n",
    "    # 機率大於設定值，則放入候選名單\n",
    "    if prob >= MIN_CONFIDENCE:\n",
    "        # 只偵測自行車(671)\n",
    "        if imagenetID != 671: continue # bike\n",
    "        # 放入候選名單\n",
    "        box = locs[i]\n",
    "        L = labels.get(label, [])\n",
    "        L.append((box, prob))\n",
    "        labels[label] = L\n",
    "\n",
    "labels.keys()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((96, 0, 346, 250), 0.4288721),\n",
       " ((144, 0, 394, 250), 0.54287004),\n",
       " ((176, 0, 426, 250), 0.5239542),\n",
       " ((192, 0, 442, 250), 0.6552249),\n",
       " ((208, 0, 458, 250), 0.5453254),\n",
       " ((224, 0, 474, 250), 0.7406801),\n",
       " ((240, 0, 490, 250), 0.5758124),\n",
       " ((256, 0, 506, 250), 0.7529945),\n",
       " ((272, 0, 522, 250), 0.754434),\n",
       " ((288, 0, 538, 250), 0.4320701),\n",
       " ((256, 32, 506, 282), 0.50544006),\n",
       " ((96, 48, 346, 298), 0.4094607),\n",
       " ((224, 48, 474, 298), 0.4887982),\n",
       " ((256, 48, 506, 298), 0.6637268),\n",
       " ((272, 48, 522, 298), 0.5355899),\n",
       " ((64, 64, 314, 314), 0.44741413),\n",
       " ((192, 64, 442, 314), 0.47727758),\n",
       " ((240, 64, 490, 314), 0.42230895),\n",
       " ((256, 64, 506, 314), 0.47051966),\n",
       " ((272, 64, 522, 314), 0.6014961),\n",
       " ((32, 80, 282, 330), 0.4938396),\n",
       " ((64, 80, 314, 330), 0.5363124),\n",
       " ((192, 80, 442, 330), 0.41979063),\n",
       " ((256, 80, 506, 330), 0.52431613),\n",
       " ((272, 80, 522, 330), 0.64922184),\n",
       " ((16, 96, 266, 346), 0.51985323),\n",
       " ((32, 96, 282, 346), 0.47494963),\n",
       " ((64, 96, 314, 346), 0.5265132),\n",
       " ((96, 96, 346, 346), 0.48602563),\n",
       " ((176, 96, 426, 346), 0.40885028),\n",
       " ((192, 96, 442, 346), 0.41327676),\n",
       " ((208, 96, 458, 346), 0.42442143),\n",
       " ((240, 96, 490, 346), 0.6908611),\n",
       " ((256, 96, 506, 346), 0.5623586),\n",
       " ((272, 96, 522, 346), 0.8143228),\n",
       " ((32, 112, 282, 362), 0.4887925),\n",
       " ((64, 112, 314, 362), 0.49131995),\n",
       " ((192, 112, 442, 362), 0.5474027),\n",
       " ((224, 112, 474, 362), 0.43801266),\n",
       " ((240, 112, 490, 362), 0.46452227),\n",
       " ((256, 112, 506, 362), 0.45976505),\n",
       " ((272, 112, 522, 362), 0.4145222),\n",
       " ((0, 128, 250, 378), 0.47673446),\n",
       " ((16, 128, 266, 378), 0.4761334),\n",
       " ((32, 128, 282, 378), 0.5771192),\n",
       " ((176, 128, 426, 378), 0.6788702),\n",
       " ((208, 128, 458, 378), 0.43918636),\n",
       " ((240, 128, 490, 378), 0.7288562),\n",
       " ((256, 128, 506, 378), 0.55933183),\n",
       " ((272, 128, 522, 378), 0.562822),\n",
       " ((32, 144, 282, 394), 0.4982727),\n",
       " ((64, 144, 314, 394), 0.6089857),\n",
       " ((176, 144, 426, 394), 0.6038735),\n",
       " ((192, 144, 442, 394), 0.71978205),\n",
       " ((224, 144, 474, 394), 0.588633),\n",
       " ((240, 144, 490, 394), 0.6049126),\n",
       " ((256, 144, 506, 394), 0.7958361),\n",
       " ((272, 144, 522, 394), 0.82800925),\n",
       " ((288, 144, 538, 394), 0.58653283),\n",
       " ((320, 144, 570, 394), 0.45551306),\n",
       " ((0, 0, 375, 375), 0.4758868),\n",
       " ((24, 0, 399, 375), 0.6157716),\n",
       " ((48, 0, 423, 375), 0.41032168),\n",
       " ((72, 0, 447, 375), 0.5487216),\n",
       " ((96, 0, 471, 375), 0.55643773),\n",
       " ((120, 0, 495, 375), 0.57208204)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels['mountain bike']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定義NMS函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://learnopencv.com/non-maximum-suppression-theory-and-implementation-in-pytorch/\n",
    "def nms_pytorch(P ,thresh_iou):\n",
    "    # we extract coordinates for every \n",
    "    # prediction box present in P\n",
    "    x1 = P[:, 0]\n",
    "    y1 = P[:, 1]\n",
    "    x2 = P[:, 2]\n",
    "    y2 = P[:, 3]\n",
    "\n",
    "    # we extract the confidence scores as well\n",
    "    scores = P[:, 4]\n",
    "\n",
    "    # calculate area of every block in P\n",
    "    areas = (x2 - x1) * (y2 - y1)\n",
    "    \n",
    "    # sort the prediction boxes in P\n",
    "    # according to their confidence scores\n",
    "    order = scores.argsort()\n",
    "\n",
    "    # initialise an empty list for \n",
    "    # filtered prediction boxes\n",
    "    keep = []\n",
    "    \n",
    "\n",
    "    while len(order) > 0:\n",
    "        \n",
    "        # extract the index of the \n",
    "        # prediction with highest score\n",
    "        # we call this prediction S\n",
    "        idx = order[-1]\n",
    "\n",
    "        # push S in filtered predictions list\n",
    "        keep.append(P[idx])\n",
    "\n",
    "        # remove S from P\n",
    "        order = order[:-1]\n",
    "\n",
    "        # sanity check\n",
    "        if len(order) == 0:\n",
    "            break\n",
    "        \n",
    "        # select coordinates of BBoxes according to \n",
    "        # the indices in order\n",
    "        xx1 = torch.index_select(x1,dim = 0, index = order)\n",
    "        xx2 = torch.index_select(x2,dim = 0, index = order)\n",
    "        yy1 = torch.index_select(y1,dim = 0, index = order)\n",
    "        yy2 = torch.index_select(y2,dim = 0, index = order)\n",
    "\n",
    "        # find the coordinates of the intersection boxes\n",
    "        xx1 = torch.max(xx1, x1[idx])\n",
    "        yy1 = torch.max(yy1, y1[idx])\n",
    "        xx2 = torch.min(xx2, x2[idx])\n",
    "        yy2 = torch.min(yy2, y2[idx])\n",
    "\n",
    "        # find height and width of the intersection boxes\n",
    "        w = xx2 - xx1\n",
    "        h = yy2 - yy1\n",
    "        \n",
    "        # take max with 0.0 to avoid negative w and h\n",
    "        # due to non-overlapping boxes\n",
    "        w = torch.clamp(w, min=0.0)\n",
    "        h = torch.clamp(h, min=0.0)\n",
    "\n",
    "        # find the intersection area\n",
    "        inter = w*h\n",
    "\n",
    "        # find the areas of BBoxes according the indices in order\n",
    "        rem_areas = torch.index_select(areas, dim = 0, index = order) \n",
    "\n",
    "        # find the union of every prediction T in P\n",
    "        # with the prediction S\n",
    "        # Note that areas[idx] represents area of S\n",
    "        union = (rem_areas - inter) + areas[idx]\n",
    "        \n",
    "        # find the IoU of every prediction in P with S\n",
    "        IoU = inter / union\n",
    "\n",
    "        # keep the boxes with IoU less than thresh_iou\n",
    "        mask = IoU < thresh_iou\n",
    "        order = order[mask]\n",
    "    \n",
    "    return keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression_slow(boxes, overlapThresh=0.5):\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "    \n",
    "    pick = []        # 儲存篩選的結果\n",
    "    x1 = boxes[:,0]  # 取得候選的視窗的左/上/右/下 座標\n",
    "    y1 = boxes[:,1]\n",
    "    x2 = boxes[:,2]\n",
    "    y2 = boxes[:,3]\n",
    "    \n",
    "    # 計算候選視窗的面積\n",
    "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    idxs = np.argsort(y2)   # 依視窗的底Y座標排序\n",
    "    \n",
    "    # 比對重疊比例\n",
    "    while len(idxs) > 0:\n",
    "        # 最後一筆\n",
    "        last = len(idxs) - 1\n",
    "        i = idxs[last]\n",
    "        pick.append(i)\n",
    "        suppress = [last]\n",
    "        \n",
    "        # 比對最後一筆與其他視窗重疊的比例\n",
    "        for pos in range(0, last):\n",
    "            j = idxs[pos]\n",
    "            \n",
    "            # 取得所有視窗的涵蓋範圍\n",
    "            xx1 = max(x1[i], x1[j])\n",
    "            yy1 = max(y1[i], y1[j])\n",
    "            xx2 = min(x2[i], x2[j])\n",
    "            yy2 = min(y2[i], y2[j])\n",
    "            w = max(0, xx2 - xx1 + 1)\n",
    "            h = max(0, yy2 - yy1 + 1)\n",
    "            \n",
    "            # 計算重疊比例\n",
    "            overlap = float(w * h) / area[j]\n",
    "            \n",
    "            # 如果大於門檻值，則儲存起來\n",
    "            if overlap > overlapThresh:\n",
    "                suppress.append(pos)\n",
    "                \n",
    "        # 刪除合格的視窗，繼續比對\n",
    "        idxs = np.delete(idxs, suppress)\n",
    "        \n",
    "    # 傳回合格的視窗\n",
    "    return boxes[pick]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 進行 NMS，並對偵測到的物件畫框"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 掃描每一個類別\n",
    "for label in labels.keys():\n",
    "    #if label != categories[671]: continue # bike\n",
    "    \n",
    "    # 複製原圖\n",
    "    open_cv_image = PIL2CV2(orig) \n",
    "\n",
    "    # 畫框\n",
    "    for (box, prob) in labels[label]:\n",
    "        (startX, startY, endX, endY) = box\n",
    "        cv2.rectangle(open_cv_image, (startX, startY), (endX, endY),\n",
    "            (0, 255, 0), 2)\n",
    "\n",
    "    # 顯示 NMS(non-maxima suppression) 前的框\n",
    "    cv2.imshow(\"Before NMS\", open_cv_image)\n",
    "\n",
    "    # NMS\n",
    "    open_cv_image2 = PIL2CV2(orig) \n",
    "    boxes = np.array([p[0] for p in labels[label]])\n",
    "    proba = np.array([p[1] for p in labels[label]])\n",
    "    # print(boxes.shape, proba.shape)\n",
    "    # boxes = nms_pytorch(torch.cat((torch.tensor(boxes), \n",
    "    #    torch.tensor(proba).reshape(proba.shape[0], -1)), dim=1) , \n",
    "    #    MIN_CONFIDENCE) # non max suppression\n",
    "    boxes = non_max_suppression_slow(boxes, MIN_CONFIDENCE) # non max suppression\n",
    "    \n",
    "    color_list=[(0, 255, 0), (255, 0, 0), (255, 255, 0), (0, 0, 0), (0, 255, 255)]\n",
    "    for i, x in enumerate(boxes):\n",
    "        # startX, startY, endX, endY, label = x.numpy()\n",
    "        startX, startY, endX, endY = x #.numpy()\n",
    "        # 畫框及類別\n",
    "        cv2.rectangle(open_cv_image2, (int(startX), int(startY)), (int(endX), int(endY))\n",
    "                      , color_list[i%len(color_list)], 2)\n",
    "        startY = startY - 15 if startY - 15 > 0 else startY + 15\n",
    "        cv2.putText(open_cv_image2, str(label), (int(startX), int(startY)),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    "\n",
    "    # 顯示\n",
    "    cv2.imshow(\"After NMS\", open_cv_image2)\n",
    "    cv2.waitKey(0)\n",
    "            \n",
    "cv2.destroyAllWindows()    # 關閉所有視窗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
