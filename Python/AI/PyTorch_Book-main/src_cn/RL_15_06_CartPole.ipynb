{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 木棒台车(CartPole)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 载入相关套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import envs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机行动"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数设定\n",
    "no = 50        # 比赛回合数\n",
    "\n",
    "# 载入木棒台车(CartPole)游戏\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# 重置\n",
    "observation = env.reset()\n",
    "all_rewards=[] # 每回合总报酬\n",
    "all_steps=[] # 每回合总步数\n",
    "total_rewards = 0\n",
    "total_steps=0\n",
    "\n",
    "while no > 0:   # 执行 50 比赛回合数\n",
    "    # 随机行动\n",
    "    action = env.action_space.sample() \n",
    "    total_steps+=1\n",
    "\n",
    "    # 触动下一步\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    # 累计报酬\n",
    "    total_rewards += reward\n",
    "\n",
    "    # 比赛回合结束，重置\n",
    "    if done:\n",
    "        observation = env.reset()\n",
    "        all_rewards.append(total_rewards)\n",
    "        all_steps.append(total_steps)\n",
    "        total_rewards = 0\n",
    "        total_steps=0\n",
    "        no-=1\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "回合\t报酬\t结果\n",
      "0\t26.0\tLoss\n",
      "1\t71.0\tLoss\n",
      "2\t13.0\tLoss\n",
      "3\t11.0\tLoss\n",
      "4\t13.0\tLoss\n",
      "5\t19.0\tLoss\n",
      "6\t12.0\tLoss\n",
      "7\t11.0\tLoss\n",
      "8\t31.0\tLoss\n",
      "9\t19.0\tLoss\n",
      "10\t44.0\tLoss\n",
      "11\t13.0\tLoss\n",
      "12\t17.0\tLoss\n",
      "13\t29.0\tLoss\n",
      "14\t29.0\tLoss\n",
      "15\t40.0\tLoss\n",
      "16\t27.0\tLoss\n",
      "17\t25.0\tLoss\n",
      "18\t12.0\tLoss\n",
      "19\t13.0\tLoss\n",
      "20\t20.0\tLoss\n",
      "21\t14.0\tLoss\n",
      "22\t43.0\tLoss\n",
      "23\t14.0\tLoss\n",
      "24\t15.0\tLoss\n",
      "25\t23.0\tLoss\n",
      "26\t20.0\tLoss\n",
      "27\t12.0\tLoss\n",
      "28\t12.0\tLoss\n",
      "29\t46.0\tLoss\n",
      "30\t13.0\tLoss\n",
      "31\t18.0\tLoss\n",
      "32\t19.0\tLoss\n",
      "33\t15.0\tLoss\n",
      "34\t27.0\tLoss\n",
      "35\t8.0\tLoss\n",
      "36\t47.0\tLoss\n",
      "37\t20.0\tLoss\n",
      "38\t42.0\tLoss\n",
      "39\t11.0\tLoss\n",
      "40\t47.0\tLoss\n",
      "41\t14.0\tLoss\n",
      "42\t29.0\tLoss\n",
      "43\t23.0\tLoss\n",
      "44\t27.0\tLoss\n",
      "45\t10.0\tLoss\n",
      "46\t28.0\tLoss\n",
      "47\t20.0\tLoss\n",
      "48\t10.0\tLoss\n",
      "49\t12.0\tLoss\n"
     ]
    }
   ],
   "source": [
    "# 显示执行结果\n",
    "print('回合\\t报酬\\t结果')\n",
    "for i, (rewards, steps) in enumerate(zip(all_rewards, all_steps)):\n",
    "    result = 'Win' if steps >= 200 else 'Loss'\n",
    "    print(f'{i}\\t{rewards}\\t{result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 传统解法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "# 参数设定\n",
    "left, right = 0, 1  # 台车行进方向\n",
    "max_angle = 8       # 偏右8度以上，就往右前进，偏左也是同样处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    # 初始化\n",
    "    def __init__(self):\n",
    "        self.direction = left\n",
    "        self.last_direction=right\n",
    "        \n",
    "    # 自订策略\n",
    "    def act(self, observation):\n",
    "        # 台车位置、台车速度、平衡杆角度、平衡杆速度\n",
    "        cart_position, cart_velocity, pole_angle, pole_velocity = observation\n",
    "        \n",
    "        '''\n",
    "        行动策略：\n",
    "        1. 设定每次行动采一左一右，尽量不离中心点。\n",
    "        2. 平衡杆角度偏右8度以上，就往右前进，直到角度偏右小于8度。\n",
    "        3. 反之，偏左也是同样处理。\n",
    "        '''\n",
    "        if pole_angle < math.radians(max_angle) and \\\n",
    "            pole_angle > math.radians(-max_angle):\n",
    "            self.direction = (self.last_direction + 1) % 2\n",
    "        elif pole_angle >= math.radians(max_angle):\n",
    "            self.direction = right\n",
    "        else:\n",
    "            self.direction = left\n",
    "\n",
    "        self.last_direction = self.direction\n",
    "        \n",
    "        return self.direction  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "no = 50        # 比赛回合数\n",
    "\n",
    "# 载入 木棒台车(CartPole) 游戏\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# 重置\n",
    "observation = env.reset()\n",
    "all_rewards=[] # 每回合总报酬\n",
    "all_steps=[] # 每回合总步数\n",
    "total_rewards = 0\n",
    "total_steps=0\n",
    "\n",
    "agent = Agent()\n",
    "while no > 0:   # 执行 50 比赛回合数\n",
    "    # 行动\n",
    "    action = agent.act(observation) #env.action_space.sample()\n",
    "    total_steps+=1\n",
    "\n",
    "    # 触动下一步\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    # 累计报酬\n",
    "    total_rewards += reward\n",
    "\n",
    "    # 比赛回合结束，重置\n",
    "    if done:\n",
    "        observation = env.reset()\n",
    "        all_rewards.append(total_rewards)\n",
    "        total_rewards = 0\n",
    "        all_steps.append(total_steps)\n",
    "        total_steps = 0\n",
    "        no-=1\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "回合\t报酬\t结果\n",
      "0\t70.0\tLoss\n",
      "1\t66.0\tLoss\n",
      "2\t78.0\tLoss\n",
      "3\t136.0\tLoss\n",
      "4\t130.0\tLoss\n",
      "5\t130.0\tLoss\n",
      "6\t91.0\tLoss\n",
      "7\t111.0\tLoss\n",
      "8\t99.0\tLoss\n",
      "9\t132.0\tLoss\n",
      "10\t103.0\tLoss\n",
      "11\t102.0\tLoss\n",
      "12\t54.0\tLoss\n",
      "13\t130.0\tLoss\n",
      "14\t138.0\tLoss\n",
      "15\t69.0\tLoss\n",
      "16\t65.0\tLoss\n",
      "17\t130.0\tLoss\n",
      "18\t100.0\tLoss\n",
      "19\t106.0\tLoss\n",
      "20\t132.0\tLoss\n",
      "21\t62.0\tLoss\n",
      "22\t70.0\tLoss\n",
      "23\t92.0\tLoss\n",
      "24\t123.0\tLoss\n",
      "25\t71.0\tLoss\n",
      "26\t70.0\tLoss\n",
      "27\t101.0\tLoss\n",
      "28\t57.0\tLoss\n",
      "29\t101.0\tLoss\n",
      "30\t97.0\tLoss\n",
      "31\t66.0\tLoss\n",
      "32\t88.0\tLoss\n",
      "33\t88.0\tLoss\n",
      "34\t142.0\tLoss\n",
      "35\t136.0\tLoss\n",
      "36\t124.0\tLoss\n",
      "37\t43.0\tLoss\n",
      "38\t83.0\tLoss\n",
      "39\t108.0\tLoss\n",
      "40\t86.0\tLoss\n",
      "41\t102.0\tLoss\n",
      "42\t97.0\tLoss\n",
      "43\t83.0\tLoss\n",
      "44\t41.0\tLoss\n",
      "45\t185.0\tLoss\n",
      "46\t149.0\tLoss\n",
      "47\t75.0\tLoss\n",
      "48\t72.0\tLoss\n",
      "49\t114.0\tLoss\n",
      "50\t121.0\tLoss\n",
      "51\t103.0\tLoss\n",
      "52\t76.0\tLoss\n",
      "53\t72.0\tLoss\n",
      "54\t107.0\tLoss\n",
      "55\t45.0\tLoss\n",
      "56\t127.0\tLoss\n",
      "57\t89.0\tLoss\n",
      "58\t81.0\tLoss\n",
      "59\t98.0\tLoss\n",
      "60\t159.0\tLoss\n",
      "61\t104.0\tLoss\n",
      "62\t125.0\tLoss\n",
      "63\t94.0\tLoss\n",
      "64\t100.0\tLoss\n",
      "65\t128.0\tLoss\n",
      "66\t137.0\tLoss\n",
      "67\t144.0\tLoss\n",
      "68\t108.0\tLoss\n",
      "69\t89.0\tLoss\n",
      "70\t113.0\tLoss\n",
      "71\t140.0\tLoss\n",
      "72\t83.0\tLoss\n",
      "73\t70.0\tLoss\n",
      "74\t114.0\tLoss\n",
      "75\t121.0\tLoss\n",
      "76\t90.0\tLoss\n",
      "77\t69.0\tLoss\n",
      "78\t80.0\tLoss\n",
      "79\t81.0\tLoss\n",
      "80\t121.0\tLoss\n",
      "81\t71.0\tLoss\n",
      "82\t62.0\tLoss\n",
      "83\t104.0\tLoss\n",
      "84\t103.0\tLoss\n",
      "85\t89.0\tLoss\n",
      "86\t65.0\tLoss\n",
      "87\t95.0\tLoss\n",
      "88\t128.0\tLoss\n",
      "89\t43.0\tLoss\n",
      "90\t53.0\tLoss\n",
      "91\t99.0\tLoss\n",
      "92\t65.0\tLoss\n",
      "93\t113.0\tLoss\n",
      "94\t49.0\tLoss\n",
      "95\t74.0\tLoss\n",
      "96\t78.0\tLoss\n",
      "97\t101.0\tLoss\n",
      "98\t73.0\tLoss\n",
      "99\t53.0\tLoss\n"
     ]
    }
   ],
   "source": [
    "# 显示执行结果\n",
    "print('回合\\t报酬\\t结果')\n",
    "for i, (rewards, steps) in enumerate(zip(all_rewards, all_steps)):\n",
    "    result = 'Win' if steps >= 200 else 'Loss'\n",
    "    print(f'{i}\\t{rewards}\\t{result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以下程式来自：[‘From Scratch_ AI Balancing Act in 50 Lines of Python’](https://towardsdatascience.com/from-scratch-ai-balancing-act-in-50-lines-of-python-7ea67ef717)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "def play(env, policy):\n",
    "    observation = env.reset()\n",
    "    \n",
    "    done = False\n",
    "    score = 0\n",
    "    observations = []\n",
    "    \n",
    "    # 训练5000步\n",
    "    for _ in range(5000):\n",
    "        observations += [observation.tolist()] # 记录历次状态\n",
    "        \n",
    "        if done: # 回合是否胜负已分\n",
    "            break\n",
    "                \n",
    "        # 行动策略选择\n",
    "        outcome = np.dot(policy, observation)\n",
    "        action = 1 if outcome > 0 else 0\n",
    "        \n",
    "        # 触发下一步\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "\n",
    "    return score, observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4980173 , 0.67918521, 0.10102454, 0.29095686]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Score 500.0\n"
     ]
    }
   ],
   "source": [
    "# 训练 10 回合\n",
    "max = (0, [], [])\n",
    "for _ in range(10):\n",
    "    policy = np.random.rand(1,4) # 产生4个随机变数 [0, 1)\n",
    "    score, observations = play(env, policy) # 开始玩\n",
    "    \n",
    "    if score > max[0]: # 取最大分数\n",
    "        max = (score, observations, policy)\n",
    "\n",
    "print('Max Score', max[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Score 500.0\n"
     ]
    }
   ],
   "source": [
    "# 最终版本\n",
    "max = (0, [], [])\n",
    "\n",
    "for _ in range(100): # 训练 100 回合    \n",
    "    policy = np.random.rand(1,4) - 0.5  # 改为 [-0.5, 0.5]\n",
    "    score, observations = play(env, policy)\n",
    "    \n",
    "    if score > max[0]:  # 取最大分数\n",
    "        max = (score, observations, policy)\n",
    "        \n",
    "print('Max Score', max[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以最大分数的policy进行实验，验证最佳策略是否有效"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.05171018, -0.0624274 ,  0.23256838,  0.22154222]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取得最佳策略\n",
    "policy = max[2]\n",
    "policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以最佳策略取代随机policy，进行 10 回合验证    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  500.0\n",
      "Score:  210.0\n",
      "Score:  500.0\n",
      "Score:  205.0\n",
      "Score:  143.0\n",
      "Score:  500.0\n",
      "Score:  500.0\n",
      "Score:  215.0\n",
      "Score:  500.0\n",
      "Score:  500.0\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10): \n",
    "    score, observations = play(env, policy)\n",
    "    print('Score: ', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
