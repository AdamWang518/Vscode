{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77bd4cb4",
   "metadata": {},
   "source": [
    "# Word2Vec in Pytorch\n",
    "### 程式来自[Word2Vec in Pytorch - Continuous Bag of Words and Skipgrams](https://srijithr.gitlab.io/post/word2vec/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d451a864",
   "metadata": {},
   "source": [
    "## CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442fdfde",
   "metadata": {},
   "source": [
    "<img src='./nlp_data/Cbow.png' width=500 align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff693d73",
   "metadata": {},
   "source": [
    "## 载入相关套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "538a7d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d91e603",
   "metadata": {},
   "source": [
    "## 参数设定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "98830937",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)  # 固定乱数种子\n",
    "CONTEXT_SIZE = 3      # 上下文个数\n",
    "EMBEDDING_DIM = 10    # 嵌入层输出维度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cffdf5",
   "metadata": {},
   "source": [
    "## 文字处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3664b410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以值(value)找键值(key)\n",
    "def get_key(word_id):\n",
    "    for key,val in word_to_ix.items():\n",
    "        if(val == word_id):\n",
    "            return key\n",
    "    return ''\n",
    "\n",
    "# 分词及前置处理        \n",
    "def read_data(file_path, remove_stopwords = False):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    if file_path.lower().startswith('http'):\n",
    "        data = urllib.request.urlopen(file_path)\n",
    "        data = data.read().decode('utf8')\n",
    "    else:\n",
    "        data = open(file_path, encoding='utf8').read()\n",
    "    tokenized_data = word_tokenize(data)\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "    else:\n",
    "        stop_words = set([])\n",
    "    stop_words.update(['.',',',':',';','(',')','#','--','...','\"'])\n",
    "    cleaned_words = [ i for i in tokenized_data if i not in stop_words ]\n",
    "    return(cleaned_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ffe490",
   "metadata": {},
   "source": [
    "## 测试本文(Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "49e2c8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = read_data('./nlp_data/word2vec_test.txt')\n",
    "\n",
    "# 或读取其他档案\n",
    "#test_sentence = 'https://www.gutenberg.org/files/57884/57884-0.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6301e8",
   "metadata": {},
   "source": [
    "## N-grams 处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0f5e624a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Empathy', 'for', 'the'], 'poor') (['for', 'the', 'poor'], 'may')\n"
     ]
    }
   ],
   "source": [
    "ngrams = []\n",
    "for i in range(len(test_sentence) - CONTEXT_SIZE):\n",
    "    tup = [test_sentence[j] for j in np.arange(i , i + CONTEXT_SIZE) ]\n",
    "    ngrams.append((tup,test_sentence[i + CONTEXT_SIZE]))\n",
    "\n",
    "print(ngrams[0], ngrams[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b97e7e8",
   "metadata": {},
   "source": [
    "## 词汇表设定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "77c27afa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "单字个数： 192\n"
     ]
    }
   ],
   "source": [
    "# 取得词汇表(vocabulary)\n",
    "vocab = set(test_sentence)\n",
    "print(\"单字个数：\",len(vocab))\n",
    "\n",
    "# 建立字典，以单字取得代码\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422058de",
   "metadata": {},
   "source": [
    "## CBOW 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "570bb667",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWModeler(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(CBOWModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # embeds -> linear -> relu -> linear -> log_softmax\n",
    "        embeds = self.embeddings(inputs).view((1, -1))  \n",
    "        out1 = F.relu(self.linear1(embeds))\n",
    "        out2 = self.linear2(out1)           \n",
    "        log_probs = F.log_softmax(out2, dim=1)\n",
    "        return log_probs\n",
    "\n",
    "    def predict(self,input):\n",
    "        # 以上下文预测\n",
    "        context_idxs = torch.LongTensor([word_to_ix[w] for w in input])\n",
    "        res = self.forward(context_idxs)\n",
    "        res_arg = torch.argmax(res)\n",
    "        res_val, res_ind = res.sort(descending=True)\n",
    "        res_val = res_val[0][:3]  # 前3个预测值\n",
    "        res_ind = res_ind[0][:3]  # 前3个预测索引值\n",
    "        for arg in zip(res_val,res_ind):\n",
    "            print([(key,val,arg[0]) for key,val in word_to_ix.items() \n",
    "                                           if val == arg[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc42d019",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a19ef670",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = CBOWModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(400):\n",
    "    total_loss = 0\n",
    "    for context, target in ngrams:\n",
    "        # 以单字取得代码\n",
    "        context_idxs = torch.LongTensor([word_to_ix[w] for w in context])\n",
    "        \n",
    "        # 梯度下降\n",
    "        model.zero_grad()\n",
    "        log_probs = model(context_idxs)\n",
    "        loss = loss_function(log_probs, torch.LongTensor([word_to_ix[target]]))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984fb6df",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8440b41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('afflictions', 14, tensor(-0.0383, grad_fn=<UnbindBackward0>))]\n",
      "[('it', 111, tensor(-4.5229, grad_fn=<UnbindBackward0>))]\n",
      "[('neither', 20, tensor(-4.7311, grad_fn=<UnbindBackward0>))]\n"
     ]
    }
   ],
   "source": [
    "model.predict(['of','all','human'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5629b4",
   "metadata": {},
   "source": [
    "## Skip-gram "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b31149",
   "metadata": {},
   "source": [
    "<img src='./nlp_data/Skip-gram.png' width=500 align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dc6eac",
   "metadata": {},
   "source": [
    "## N-grams 处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5f96b809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Empathy', ['for', 'the', 'poor']) ('for', ['the', 'poor', 'may'])\n"
     ]
    }
   ],
   "source": [
    "ngrams = []\n",
    "for i in range(len(test_sentence) - CONTEXT_SIZE):\n",
    "    tup = [test_sentence[j] for j in np.arange(i + 1 , i + CONTEXT_SIZE + 1) ]\n",
    "    ngrams.append((test_sentence[i],tup))\n",
    "print(ngrams[0], ngrams[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6356134d",
   "metadata": {},
   "source": [
    "## Skip-Gram 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "dd1aaead",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipgramModeler(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(SkipgramModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, context_size * vocab_size)\n",
    "        #self.parameters['context_size'] = context_size\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # embeds -> linear -> relu -> linear -> log_softmax\n",
    "        embeds = self.embeddings(inputs).view((1, -1)) \n",
    "        out1 = F.relu(self.linear1(embeds)) \n",
    "        out2 = self.linear2(out1)           \n",
    "        log_probs = F.log_softmax(out2, dim=1).view(CONTEXT_SIZE,-1)\n",
    "        return log_probs\n",
    "\n",
    "    def predict(self,input):\n",
    "        context_idxs = torch.LongTensor([word_to_ix[input]])\n",
    "        res = self.forward(context_idxs)\n",
    "        res_arg = torch.argmax(res)\n",
    "        res_val, res_ind = res.sort(descending=True)\n",
    "        indices = [res_ind[i][0] for i in np.arange(0,3)]\n",
    "        for arg in indices:\n",
    "            print([(key, val) for key,val in word_to_ix.items() \n",
    "                   if val == arg ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7c6151",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b251394a",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = SkipgramModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# Freeze embedding layer\n",
    "#model.freeze_layer('embeddings')\n",
    "\n",
    "for epoch in range(550):\n",
    "    total_loss = 0\n",
    "    # model.predict('psychologically')\n",
    "\n",
    "    for context, target in ngrams:\n",
    "        context_idxs = torch.LongTensor([word_to_ix[context]])\n",
    "        model.zero_grad()\n",
    "        log_probs = model(context_idxs)\n",
    "        target_list = torch.LongTensor([word_to_ix[w] for w in target])\n",
    "        loss = loss_function(log_probs, target_list)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2acab6",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0a0bf5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('and', 98)]\n",
      "[('physically', 152)]\n",
      "[('incapacitating', 169)]\n"
     ]
    }
   ],
   "source": [
    "model.predict('psychologically')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17c6534",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
