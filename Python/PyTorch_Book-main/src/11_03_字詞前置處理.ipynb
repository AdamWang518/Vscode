{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 字詞前置處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入相關套件\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試文章段落\n",
    "text=\"Today is a great day. It is even better than yesterday.\" + \\\n",
    "     \" And yesterday was the best day ever.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分割字句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Today is a great day.',\n",
       " 'It is even better than yesterday.',\n",
       " 'And yesterday was the best day ever.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分割字句\n",
    "nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Today',\n",
       " 'is',\n",
       " 'a',\n",
       " 'great',\n",
       " 'day',\n",
       " '.',\n",
       " 'It',\n",
       " 'is',\n",
       " 'even',\n",
       " 'better',\n",
       " 'than',\n",
       " 'yesterday',\n",
       " '.',\n",
       " 'And',\n",
       " 'yesterday',\n",
       " 'was',\n",
       " 'the',\n",
       " 'best',\n",
       " 'day',\n",
       " 'ever',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分詞\n",
    "nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 詞形還原"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My system keep crash hi crash yesterday, our crash daili'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 字根詞形還原(Stemming)\n",
    "text = 'My system keeps crashing his crashed yesterday, ours crashes daily'\n",
    "ps = nltk.porter.PorterStemmer()\n",
    "' '.join([ps.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My system keep crashing his crashed yesterday, ours crash daily'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 依字典規則的詞形還原(Lemmatization)\n",
    "text = 'My system keeps crashing his crashed yesterday, ours crashes daily'\n",
    "lem = nltk.WordNetLemmatizer()\n",
    "' '.join([lem.lemmatize(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 停用詞(Stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "標點符號: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Today great day It even better yesterday And yesterday best day ever'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 標點符號(Punctuation)\n",
    "import string\n",
    "print('標點符號:', string.punctuation)\n",
    "\n",
    "# 測試文章段落\n",
    "text=\"Today is a great day. It is even better than yesterday.\" + \\\n",
    "     \" And yesterday was the best day ever.\"\n",
    "# 讀取停用詞\n",
    "stopword_list = set(nltk.corpus.stopwords.words('english') \n",
    "                    + list(string.punctuation))\n",
    "\n",
    "# 移除停用詞(Removing Stopwords)\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    if is_lower_case:\n",
    "        text = text.lower()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text, filtered_tokens\n",
    "\n",
    "filtered_text, filtered_tokens = remove_stopwords(text) \n",
    "filtered_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW 測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('’', 35), ('stores', 15), ('convenience', 14), ('one', 8), ('—', 8), ('even', 8), ('seoul', 8), ('city', 7), ('korea', 6), ('korean', 6), ('cities', 6), ('people', 5), ('summer', 4), ('new', 4), ('also', 4), ('find', 4), ('store', 4), ('would', 4), ('like', 4), ('average', 4)]\n"
     ]
    }
   ],
   "source": [
    "# 測試文章段落\n",
    "with open('./NLP_data/news.txt','r+', encoding='UTF-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "filtered_text, filtered_tokens = remove_stopwords(text, True) \n",
    "\n",
    "import collections\n",
    "# 生字表的集合\n",
    "word_freqs = collections.Counter()\n",
    "for word in filtered_tokens:\n",
    "    word_freqs[word] += 1\n",
    "print(word_freqs.most_common(20))         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('store', 19), ('convenience', 14), ('city', 13), ('one', 8), ('even', 8), ('seoul', 8), ('korea', 6), ('korean', 6), ('night', 6), ('food', 5), ('ha', 5), ('people', 5), ('summer', 4), ('new', 4), ('life', 4), ('also', 4), ('find', 4), ('would', 4), ('like', 4), ('chain', 4)]\n"
     ]
    }
   ],
   "source": [
    "# 移除停用詞(Removing Stopwords)\n",
    "lem = nltk.WordNetLemmatizer()\n",
    "def remove_stopwords_regex(text, is_lower_case=False):\n",
    "    if is_lower_case:\n",
    "        text = text.lower()\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+') # 篩選文數字(Alphanumeric)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [lem.lemmatize(token.strip()) for token in tokens] # 詞形還原\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text, filtered_tokens\n",
    "\n",
    "filtered_text, filtered_tokens = remove_stopwords_regex(text, True) \n",
    "word_freqs = collections.Counter()\n",
    "for word in filtered_tokens:\n",
    "    word_freqs[word] += 1\n",
    "print(word_freqs.most_common(20))         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'korean'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem.lemmatize('korean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 相似詞(Synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('love.n.01'),\n",
       " Synset('love.n.02'),\n",
       " Synset('beloved.n.01'),\n",
       " Synset('love.n.04'),\n",
       " Synset('love.n.05'),\n",
       " Synset('sexual_love.n.02'),\n",
       " Synset('love.v.01'),\n",
       " Synset('love.v.02'),\n",
       " Synset('love.v.03'),\n",
       " Synset('sleep_together.v.01')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 找出相似詞(Synonyms)\n",
    "synonyms = nltk.corpus.wordnet.synsets('love')\n",
    "synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a strong positive emotion of regard and affection'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 單字說明\n",
    "synonyms[0].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['his love for his work', 'children need a lot of love']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 單字的例句\n",
    "synonyms[0].examples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 相反詞(Antonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beautiful']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 找出相反詞(Antonyms)\n",
    "antonyms=[]\n",
    "for syn in nltk.corpus.wordnet.synsets('ugly'):\n",
    "    for l in syn.lemmas():\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "antonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 詞性標籤(POS Tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('am', 'VBP'), ('a', 'DT'), ('human', 'JJ'), ('being', 'VBG'), (',', ','), ('capable', 'JJ'), ('of', 'IN'), ('doing', 'VBG'), ('terrible', 'JJ'), ('things', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "# 找出詞性標籤(POS Tagging)\n",
    "text='I am a human being, capable of doing terrible things'\n",
    "sentences=nltk.sent_tokenize(text)\n",
    "for sent in sentences:\n",
    "    print(nltk.pos_tag(nltk.word_tokenize(sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
